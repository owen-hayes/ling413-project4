{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geohash</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pruzhany</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 15:13:17</td>\n",
       "      <td>@WarcraftDevs - demon spikes reflect x % of in...</td>\n",
       "      <td>u936uxwkybu2</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lumphat</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:25:06</td>\n",
       "      <td>@phuonganhh21 e</td>\n",
       "      <td>w6khf3wghq2s</td>\n",
       "      <td>kh</td>\n",
       "      <td>asia_southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsiombe</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-12-06 15:50:41</td>\n",
       "      <td>Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...</td>\n",
       "      <td>m5824frby6pt</td>\n",
       "      <td>mg</td>\n",
       "      <td>africa_sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boruny</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:55:03</td>\n",
       "      <td>@SobolLubov #–û—Å–∫–∞—Ä–ü—É—Ç–∏–Ω—É #–û—Å–∫–∞—Ä #–ü—É—Ç–∏–Ω–∏–∑–º #–ù–∞–≤...</td>\n",
       "      <td>u9dt48j9wb99</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vilkaviskis</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 17:15:42</td>\n",
       "      <td>Top rated project! @polygen_io and #Launchpad ...</td>\n",
       "      <td>u98quj5919n4</td>\n",
       "      <td>lt</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744328</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>https://t.co/cDu892Apj6</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744329</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>‰ªäÊó•„Å†„Åë„Åß4ÁéâÈ£ü„Åπ„Å¶ÂÜ∑ÂáçÂ∫´„ÇíÁ©∫„Å´„Åó„Åü„ÄÇÂºïË∂ä„ÅóÂâç„ÅÜ„Å©„Çì„ÄÇ https://t.co/zG57D...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744330</th>\n",
       "      <td>pedro betancourt</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>ÊòØÊàë‰∫Ü https://t.co/KPTyf0hJCO</td>\n",
       "      <td>dhn1q1q7ttq1</td>\n",
       "      <td>cu</td>\n",
       "      <td>america_central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744331</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>üôá‚Äç‚ôÄÔ∏èüíïüíï https://t.co/orOVDvtL85</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744332</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>@iiyohei „Åæ„Åö„ÅØÈñ¢Êù±ÔºëÈÉ®„Å´ÊòáÊ†º„Åó„Å¶Ëøé„Åà„Åü„ÅÑ„Åß„Åô„Å≠„Éº 12.25.11:00„Åã„ÇâÂÖ•Êõø„Åà...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12744333 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      City Lang                 Time  \\\n",
       "0                 pruzhany   en  2021-12-06 15:13:17   \n",
       "1                  lumphat  und  2021-12-06 15:25:06   \n",
       "2                  tsiombe   fr  2021-12-06 15:50:41   \n",
       "3                   boruny  und  2021-12-06 15:55:03   \n",
       "4              vilkaviskis   en  2021-12-06 17:15:42   \n",
       "...                    ...  ...                  ...   \n",
       "12744328              soka  und  2021-12-21 16:43:39   \n",
       "12744329              soka   ja  2021-12-21 16:43:39   \n",
       "12744330  pedro betancourt   ja  2021-12-21 16:43:39   \n",
       "12744331              soka  und  2021-12-21 16:43:40   \n",
       "12744332              soka   ja  2021-12-21 16:43:40   \n",
       "\n",
       "                                                       Text       Geohash  \\\n",
       "0         @WarcraftDevs - demon spikes reflect x % of in...  u936uxwkybu2   \n",
       "1                                           @phuonganhh21 e  w6khf3wghq2s   \n",
       "2         Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...  m5824frby6pt   \n",
       "3         @SobolLubov #–û—Å–∫–∞—Ä–ü—É—Ç–∏–Ω—É #–û—Å–∫–∞—Ä #–ü—É—Ç–∏–Ω–∏–∑–º #–ù–∞–≤...  u9dt48j9wb99   \n",
       "4         Top rated project! @polygen_io and #Launchpad ...  u98quj5919n4   \n",
       "...                                                     ...           ...   \n",
       "12744328                            https://t.co/cDu892Apj6  xn77v2fkpxu8   \n",
       "12744329  ‰ªäÊó•„Å†„Åë„Åß4ÁéâÈ£ü„Åπ„Å¶ÂÜ∑ÂáçÂ∫´„ÇíÁ©∫„Å´„Åó„Åü„ÄÇÂºïË∂ä„ÅóÂâç„ÅÜ„Å©„Çì„ÄÇ https://t.co/zG57D...  xn77v2fkpxu8   \n",
       "12744330                        ÊòØÊàë‰∫Ü https://t.co/KPTyf0hJCO  dhn1q1q7ttq1   \n",
       "12744331                     üôá‚Äç‚ôÄÔ∏èüíïüíï https://t.co/orOVDvtL85  xn77v2fkpxu8   \n",
       "12744332  @iiyohei „Åæ„Åö„ÅØÈñ¢Êù±ÔºëÈÉ®„Å´ÊòáÊ†º„Åó„Å¶Ëøé„Åà„Åü„ÅÑ„Åß„Åô„Å≠„Éº 12.25.11:00„Åã„ÇâÂÖ•Êõø„Åà...  xn77v2fkpxu8   \n",
       "\n",
       "         Country           Region  \n",
       "0             by      europe_east  \n",
       "1             kh   asia_southeast  \n",
       "2             mg       africa_sub  \n",
       "3             by      europe_east  \n",
       "4             lt      europe_east  \n",
       "...          ...              ...  \n",
       "12744328      jp        asia_east  \n",
       "12744329      jp        asia_east  \n",
       "12744330      cu  america_central  \n",
       "12744331      jp        asia_east  \n",
       "12744332      jp        asia_east  \n",
       "\n",
       "[12744333 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tweets into a DataFrame\n",
    "tweets_df = pd.read_csv(\"../project2/Data.Original.1900\")\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geohash</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pruzhany</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 15:13:17</td>\n",
       "      <td>@WarcraftDevs - demon spikes reflect x % of in...</td>\n",
       "      <td>u936uxwkybu2</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lumphat</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:25:06</td>\n",
       "      <td>@phuonganhh21 e</td>\n",
       "      <td>w6khf3wghq2s</td>\n",
       "      <td>kh</td>\n",
       "      <td>asia_southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsiombe</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-12-06 15:50:41</td>\n",
       "      <td>Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...</td>\n",
       "      <td>m5824frby6pt</td>\n",
       "      <td>mg</td>\n",
       "      <td>africa_sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boruny</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:55:03</td>\n",
       "      <td>@SobolLubov #–û—Å–∫–∞—Ä–ü—É—Ç–∏–Ω—É #–û—Å–∫–∞—Ä #–ü—É—Ç–∏–Ω–∏–∑–º #–ù–∞–≤...</td>\n",
       "      <td>u9dt48j9wb99</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vilkaviskis</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 17:15:42</td>\n",
       "      <td>Top rated project! @polygen_io and #Launchpad ...</td>\n",
       "      <td>u98quj5919n4</td>\n",
       "      <td>lt</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744328</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>https://t.co/cDu892Apj6</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744329</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>‰ªäÊó•„Å†„Åë„Åß4ÁéâÈ£ü„Åπ„Å¶ÂÜ∑ÂáçÂ∫´„ÇíÁ©∫„Å´„Åó„Åü„ÄÇÂºïË∂ä„ÅóÂâç„ÅÜ„Å©„Çì„ÄÇ https://t.co/zG57D...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744330</th>\n",
       "      <td>pedro betancourt</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>ÊòØÊàë‰∫Ü https://t.co/KPTyf0hJCO</td>\n",
       "      <td>dhn1q1q7ttq1</td>\n",
       "      <td>cu</td>\n",
       "      <td>america_central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744331</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>üôá‚Äç‚ôÄÔ∏èüíïüíï https://t.co/orOVDvtL85</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744332</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>@iiyohei „Åæ„Åö„ÅØÈñ¢Êù±ÔºëÈÉ®„Å´ÊòáÊ†º„Åó„Å¶Ëøé„Åà„Åü„ÅÑ„Åß„Åô„Å≠„Éº 12.25.11:00„Åã„ÇâÂÖ•Êõø„Åà...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12739981 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      City Lang                 Time  \\\n",
       "0                 pruzhany   en  2021-12-06 15:13:17   \n",
       "1                  lumphat  und  2021-12-06 15:25:06   \n",
       "2                  tsiombe   fr  2021-12-06 15:50:41   \n",
       "3                   boruny  und  2021-12-06 15:55:03   \n",
       "4              vilkaviskis   en  2021-12-06 17:15:42   \n",
       "...                    ...  ...                  ...   \n",
       "12744328              soka  und  2021-12-21 16:43:39   \n",
       "12744329              soka   ja  2021-12-21 16:43:39   \n",
       "12744330  pedro betancourt   ja  2021-12-21 16:43:39   \n",
       "12744331              soka  und  2021-12-21 16:43:40   \n",
       "12744332              soka   ja  2021-12-21 16:43:40   \n",
       "\n",
       "                                                       Text       Geohash  \\\n",
       "0         @WarcraftDevs - demon spikes reflect x % of in...  u936uxwkybu2   \n",
       "1                                           @phuonganhh21 e  w6khf3wghq2s   \n",
       "2         Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...  m5824frby6pt   \n",
       "3         @SobolLubov #–û—Å–∫–∞—Ä–ü—É—Ç–∏–Ω—É #–û—Å–∫–∞—Ä #–ü—É—Ç–∏–Ω–∏–∑–º #–ù–∞–≤...  u9dt48j9wb99   \n",
       "4         Top rated project! @polygen_io and #Launchpad ...  u98quj5919n4   \n",
       "...                                                     ...           ...   \n",
       "12744328                            https://t.co/cDu892Apj6  xn77v2fkpxu8   \n",
       "12744329  ‰ªäÊó•„Å†„Åë„Åß4ÁéâÈ£ü„Åπ„Å¶ÂÜ∑ÂáçÂ∫´„ÇíÁ©∫„Å´„Åó„Åü„ÄÇÂºïË∂ä„ÅóÂâç„ÅÜ„Å©„Çì„ÄÇ https://t.co/zG57D...  xn77v2fkpxu8   \n",
       "12744330                        ÊòØÊàë‰∫Ü https://t.co/KPTyf0hJCO  dhn1q1q7ttq1   \n",
       "12744331                     üôá‚Äç‚ôÄÔ∏èüíïüíï https://t.co/orOVDvtL85  xn77v2fkpxu8   \n",
       "12744332  @iiyohei „Åæ„Åö„ÅØÈñ¢Êù±ÔºëÈÉ®„Å´ÊòáÊ†º„Åó„Å¶Ëøé„Åà„Åü„ÅÑ„Åß„Åô„Å≠„Éº 12.25.11:00„Åã„ÇâÂÖ•Êõø„Åà...  xn77v2fkpxu8   \n",
       "\n",
       "         Country           Region  \n",
       "0             by      europe_east  \n",
       "1             kh   asia_southeast  \n",
       "2             mg       africa_sub  \n",
       "3             by      europe_east  \n",
       "4             lt      europe_east  \n",
       "...          ...              ...  \n",
       "12744328      jp        asia_east  \n",
       "12744329      jp        asia_east  \n",
       "12744330      cu  america_central  \n",
       "12744331      jp        asia_east  \n",
       "12744332      jp        asia_east  \n",
       "\n",
       "[12739981 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop empty rows\n",
    "tweets_df = tweets_df.dropna()\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_10000 = tweets_df[:10000]\n",
    "tweets_df_100k = tweets_df[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_emojis(text):\n",
    "    emojis = emoji.analyze(text)\n",
    "    return list(x[0] for x in emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider tweets which contain at least 1 emoji\n",
    "tweets_with_emojis = tweets_df[\n",
    "    tweets_df[\"Text\"].apply(lambda tweet_text: emoji.emoji_count(tweet_text) > 0)\n",
    "]\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column to generate 'CleanedText' column\n",
    "tweets_with_emojis[\"CleanedText\"] = tweets_with_emojis[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Get just emojis and store in 'Emojis' column\n",
    "tweets_with_emojis[\"Emojis\"] = tweets_with_emojis[\"Text\"].apply(\n",
    "    lambda text: \"\".join(get_emojis(text))\n",
    ")\n",
    "\n",
    "tweets_with_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ \n",
       "11                        üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi\n",
       "17               Hacer cosas de novios sin ser novios ‚ô•Ô∏è\n",
       "18      ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ÿßŸÅÿ∂ŸÑ ŸÅŸä ÿßÿ≥Ÿàÿ° ÿ≠...\n",
       "21                            Lmaooo i felt that shit ü§£üíÄ\n",
       "29                                            „Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åôüï∫\n",
       "30                                                   ‚ù§Ô∏è \n",
       "32                                                 üò≠üò≠üò≠üò≠ \n",
       "36     Hello!  PaidTunes is a company that pays you w...\n",
       "42                                               ŸÑÿµŸÇŸàŸà üíô\n",
       "43                                       ÿµŸÑŸä ÿπŸÑŸä ÿßŸÑŸÜÿ®Ÿä ü§ç\n",
       "51     –ó–Ω–∞–º–µ–Ω–∏—Ç—ã–π –∏–º–±–∏—Ä–Ω—ã–π —ç–ª—å üç∫ –∏–∑ –ê–Ω–≥–ª–∏–∏ üá¨üáß —Å–ª–∞–¥–∫–∏–π...\n",
       "52     –î–æ —Å–∞–º–æ–≥–æ –∫–æ–Ω—Ü–∞ —Ö–æ—á–µ—Ç—Å—è –∏–∑–±–∞–≤–∏—Ç—å –ø–∏–≤–æ üç∫ –æ—Ç —ç—Ç–æ...\n",
       "55        Me casually cleaning all my guns and bullets üòè\n",
       "57                                        D-24 ÍµøÎ™®Îãù Ïã∏ÎûëÌï¥üíô \n",
       "77                                3 shifts left at unm ü§©\n",
       "78      Yeah , we found out who lmfaoo but it threw u...\n",
       "83                       Me quede dormido en el bondiüòêüòêüòê\n",
       "87                                           combinado ü§ù\n",
       "96                                   fuckin goodmorningü•∞\n",
       "108                                        Np babes ‚ù§Ô∏è‚ù§Ô∏è\n",
       "114         nossa fui fdp e j√° sei q vem karma por a√≠ üòñüòñ\n",
       "120              acho que encontrei meu antidepressivoü™êü§ç\n",
       "123    Amo como ella me hace aterrizar, me ense√±a y m...\n",
       "126                                                  üòÇüòÇüòÇ\n",
       "127                                                    ü•∑\n",
       "131                                        Dooooooo itüëÄ \n",
       "132                                                  ‚≠êüåä \n",
       "133                                                   üî• \n",
       "135    üëánunca hay que justificar a un violento, ni el...\n",
       "140                                                üî•üî•üî•üî•üî•\n",
       "147               Actually, I need to earn not to learnüíî\n",
       "151    bakpelah guess ada sales ü•≤ü•≤ kekgi nak g jalan ...\n",
       "152    alhamdulillah dapat insentif untuk 3bulan skli...\n",
       "160             –Ω—ç–≥ –º—É—É –∞–º–±–∞–∞—Ä—à–¥—ç—ç —é—É–≥ –Ω—å —à–∞–ª–≥–∞–¥–∞–≥ —é–º ü§£ \n",
       "161    Mis amigas siempre me preguntan porque estoy t...\n",
       "163      üò¥ was to easy cooz u should run it back and ...\n",
       "169                                Ÿäÿ≤ŸÅŸàŸÜŸä ŸàÿßŸÜÿß ŸÖÿßŸÑŸä ÿ¥ÿ∫ŸÑü•≤\n",
       "173                    ÿßŸÑŸÑŸáŸÖ ÿßÿ¨ÿπŸÑŸÜÿß ŸÖŸÜ ÿ£ÿµÿ≠ÿßÿ® ÿßŸÑŸäŸÖŸäŸÜ üôè‚ô•Ô∏è \n",
       "192           Water happy actor üòÑüòÑ @ Hilton Nay Pyi Taw \n",
       "206    üè° Maison √† louer R+2  Cit√© Immobili√®re Tamatav...\n",
       "225                                            „ÅÜ„Åæ„Åô„Åé„Åß„Åô„Å≠üò≥üò≥\n",
       "226        The world is a beautiful place üó∫ #ÿµÿ®ÿßÿ≠_ÿßŸÑÿÆŸäÿ± \n",
       "227    Chufangagri #agricultural #drone spraying oper...\n",
       "228    ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ Ÿáÿ∞ÿß ÿßŸÑŸÅŸäÿØŸäŸà ŸàŸÖÿ¥ÿßÿ±ŸÉÿ™Ÿá ÿ®Ÿàÿßÿ≥ÿ∑ÿ©SNAPTUBE. ...\n",
       "235    IDO Whitelist is open: üîó  üîó  Gadget War is a P...\n",
       "252                                     Follow my tips üòÄ\n",
       "266                                   üòÇüòÇ €åÿπŸÜ€å ÿ¥ÿßŸÜÿ≥ÿ™Ÿà ...\n",
       "269     Doma laba, bet ≈°eit esi samiksƒìjis 2 grupƒìjum...\n",
       "270                         Kukumbigwa mari yeAirtimeüòÇüòÇüòÇ\n",
       "Name: CleanedText, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_emojis[:50]['CleanedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_regional_indicator_or_zwj_or_gender(char):\n",
    "    return char.encode(\"utf-8\")[:3] == b\"\\xf0\\x9f\\x87\" or char in [\n",
    "        \"\\u200d\",\n",
    "        \"\\u2640\",\n",
    "        \"\\u2642\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_emoji_sequences(text):\n",
    "    emoji_sequences = []\n",
    "    formatted_text = \"\"\n",
    "    current_sequence = \"\"\n",
    "    for i, char in enumerate(text):\n",
    "        # Check if is emoji or is regional indicator like \"üá¶\"\n",
    "        if emoji.is_emoji(char) or is_regional_indicator_or_zwj_or_gender(char):\n",
    "            current_sequence += char\n",
    "        else:\n",
    "            if current_sequence:\n",
    "                emoji_sequences.append(current_sequence)\n",
    "                if formatted_text and formatted_text[-1] != \" \":\n",
    "                    # print(\"adding space\")\n",
    "                    formatted_text += \" \"\n",
    "                formatted_text += current_sequence\n",
    "                current_sequence = \"\"\n",
    "\n",
    "            if (\n",
    "                formatted_text\n",
    "                and formatted_text[-1] != \" \"\n",
    "                and (\n",
    "                    emoji.is_emoji(formatted_text[-1])\n",
    "                    or is_regional_indicator_or_zwj_or_gender(char)\n",
    "                )\n",
    "            ):\n",
    "                formatted_text += \" \"\n",
    "            formatted_text += char\n",
    "\n",
    "    if current_sequence:\n",
    "        emoji_sequences.append(current_sequence)\n",
    "        if formatted_text and formatted_text[-1] != \" \":\n",
    "            formatted_text += \" \"\n",
    "        formatted_text += current_sequence\n",
    "\n",
    "    return formatted_text.strip(), emoji_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences\n",
    "tweets_with_emojis[\"SeparatedText\"], tweets_with_emojis[\"EmojiSequences\"] = zip(\n",
    "    *tweets_with_emojis[\"CleanedText\"].apply(extract_emoji_sequences)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geohash</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>SeparatedText</th>\n",
       "      <th>EmojiSequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsiombe</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-12-06 15:50:41</td>\n",
       "      <td>Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...</td>\n",
       "      <td>m5824frby6pt</td>\n",
       "      <td>mg</td>\n",
       "      <td>africa_sub</td>\n",
       "      <td>Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ</td>\n",
       "      <td>üíÄüíÄ</td>\n",
       "      <td>Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ</td>\n",
       "      <td>[üíÄüíÄ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hobyo</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 23:04:04</td>\n",
       "      <td>@M0zark0 üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi</td>\n",
       "      <td>t0fw2npnp7hn</td>\n",
       "      <td>so</td>\n",
       "      <td>africa_north</td>\n",
       "      <td>üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi</td>\n",
       "      <td>üò≠üò≠üò≠üò≠üò≠</td>\n",
       "      <td>üò≠üò≠üò≠üò≠üò≠  ur ugly for that wallahi</td>\n",
       "      <td>[üò≠üò≠üò≠üò≠üò≠]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ayny</td>\n",
       "      <td>es</td>\n",
       "      <td>2021-12-06 23:19:23</td>\n",
       "      <td>Hacer cosas de novios sin ser novios ‚ô•Ô∏è</td>\n",
       "      <td>tx08psjenf21</td>\n",
       "      <td>tj</td>\n",
       "      <td>asia_central</td>\n",
       "      <td>Hacer cosas de novios sin ser novios ‚ô•Ô∏è</td>\n",
       "      <td>‚ô•Ô∏è</td>\n",
       "      <td>Hacer cosas de novios sin ser novios ‚ô• Ô∏è</td>\n",
       "      <td>[‚ô•]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aflou</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-12-06 23:23:17</td>\n",
       "      <td>@beINSPORTS_news ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±...</td>\n",
       "      <td>sn16pc6ddver</td>\n",
       "      <td>dz</td>\n",
       "      <td>africa_north</td>\n",
       "      <td>ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ÿßŸÅÿ∂ŸÑ ŸÅŸä ÿßÿ≥Ÿàÿ° ÿ≠...</td>\n",
       "      <td>üòÇ</td>\n",
       "      <td>ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ÿßŸÅÿ∂ŸÑ ŸÅŸä ÿßÿ≥Ÿàÿ° ÿ≠ÿß...</td>\n",
       "      <td>[üòÇ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>villa yapacani</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 23:28:34</td>\n",
       "      <td>@andalechuey Lmaooo i felt that shit ü§£üíÄ</td>\n",
       "      <td>6sft52mvxsct</td>\n",
       "      <td>bo</td>\n",
       "      <td>america_south</td>\n",
       "      <td>Lmaooo i felt that shit ü§£üíÄ</td>\n",
       "      <td>ü§£üíÄ</td>\n",
       "      <td>Lmaooo i felt that shit ü§£üíÄ</td>\n",
       "      <td>[ü§£üíÄ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100032</th>\n",
       "      <td>bojnurd</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-12-15 10:53:44</td>\n",
       "      <td>@heyfa64 ŸÖŸÖŸÜŸàŸÜŸÖ‚ù§</td>\n",
       "      <td>tq8v0qw6k277</td>\n",
       "      <td>ir</td>\n",
       "      <td>asia_central</td>\n",
       "      <td>ŸÖŸÖŸÜŸàŸÜŸÖ‚ù§</td>\n",
       "      <td>‚ù§</td>\n",
       "      <td>ŸÖŸÖŸÜŸàŸÜŸÖ ‚ù§</td>\n",
       "      <td>[‚ù§]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100033</th>\n",
       "      <td>kaliganj</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-15 10:53:44</td>\n",
       "      <td>@Milon10941303 ‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>tupmhqbgvphc</td>\n",
       "      <td>bd</td>\n",
       "      <td>asia_south</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "      <td>[‚ù§, ‚ù§]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100035</th>\n",
       "      <td>tiszaujvaros</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-15 10:53:44</td>\n",
       "      <td>@kyloliqq ü§çü§çü§ç</td>\n",
       "      <td>u2wbxvbrvkcq</td>\n",
       "      <td>hu</td>\n",
       "      <td>europe_east</td>\n",
       "      <td>ü§çü§çü§ç</td>\n",
       "      <td>ü§çü§çü§ç</td>\n",
       "      <td>ü§çü§çü§ç</td>\n",
       "      <td>[ü§çü§çü§ç]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100039</th>\n",
       "      <td>kuusamo</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-15 10:53:45</td>\n",
       "      <td>@kiyowyd ü•∞ü•µ</td>\n",
       "      <td>uesz20fesvvy</td>\n",
       "      <td>fi</td>\n",
       "      <td>europe_west</td>\n",
       "      <td>ü•∞ü•µ</td>\n",
       "      <td>ü•∞ü•µ</td>\n",
       "      <td>ü•∞ü•µ</td>\n",
       "      <td>[ü•∞ü•µ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100040</th>\n",
       "      <td>preveza</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-15 10:53:46</td>\n",
       "      <td>@alperaltunoz @Turukku_ @FilipposBetsan1 @Akde...</td>\n",
       "      <td>sqyv84tsnn32</td>\n",
       "      <td>gr</td>\n",
       "      <td>europe_west</td>\n",
       "      <td>Stop watching Turkish soap opera.üçºü¶É</td>\n",
       "      <td>üçºü¶É</td>\n",
       "      <td>Stop watching Turkish soap opera. üçºü¶É</td>\n",
       "      <td>[üçºü¶É]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20637 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  City Lang                 Time  \\\n",
       "2              tsiombe   fr  2021-12-06 15:50:41   \n",
       "11               hobyo   en  2021-12-06 23:04:04   \n",
       "17                ayny   es  2021-12-06 23:19:23   \n",
       "18               aflou   ar  2021-12-06 23:23:17   \n",
       "21      villa yapacani   en  2021-12-06 23:28:34   \n",
       "...                ...  ...                  ...   \n",
       "100032         bojnurd   ar  2021-12-15 10:53:44   \n",
       "100033        kaliganj  und  2021-12-15 10:53:44   \n",
       "100035    tiszaujvaros  und  2021-12-15 10:53:44   \n",
       "100039         kuusamo  und  2021-12-15 10:53:45   \n",
       "100040         preveza   en  2021-12-15 10:53:46   \n",
       "\n",
       "                                                     Text       Geohash  \\\n",
       "2       Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...  m5824frby6pt   \n",
       "11                @M0zark0 üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi  t0fw2npnp7hn   \n",
       "17                Hacer cosas de novios sin ser novios ‚ô•Ô∏è  tx08psjenf21   \n",
       "18      @beINSPORTS_news ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±...  sn16pc6ddver   \n",
       "21                @andalechuey Lmaooo i felt that shit ü§£üíÄ  6sft52mvxsct   \n",
       "...                                                   ...           ...   \n",
       "100032                                   @heyfa64 ŸÖŸÖŸÜŸàŸÜŸÖ‚ù§  tq8v0qw6k277   \n",
       "100033                                @Milon10941303 ‚ù§Ô∏è‚ù§Ô∏è  tupmhqbgvphc   \n",
       "100035                                      @kyloliqq ü§çü§çü§ç  u2wbxvbrvkcq   \n",
       "100039                                        @kiyowyd ü•∞ü•µ  uesz20fesvvy   \n",
       "100040  @alperaltunoz @Turukku_ @FilipposBetsan1 @Akde...  sqyv84tsnn32   \n",
       "\n",
       "       Country         Region  \\\n",
       "2           mg     africa_sub   \n",
       "11          so   africa_north   \n",
       "17          tj   asia_central   \n",
       "18          dz   africa_north   \n",
       "21          bo  america_south   \n",
       "...        ...            ...   \n",
       "100032      ir   asia_central   \n",
       "100033      bd     asia_south   \n",
       "100035      hu    europe_east   \n",
       "100039      fi    europe_west   \n",
       "100040      gr    europe_west   \n",
       "\n",
       "                                              CleanedText Emojis  \\\n",
       "2         Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ      üíÄüíÄ   \n",
       "11                         üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi  üò≠üò≠üò≠üò≠üò≠   \n",
       "17                Hacer cosas de novios sin ser novios ‚ô•Ô∏è     ‚ô•Ô∏è   \n",
       "18       ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ÿßŸÅÿ∂ŸÑ ŸÅŸä ÿßÿ≥Ÿàÿ° ÿ≠...      üòÇ   \n",
       "21                             Lmaooo i felt that shit ü§£üíÄ     ü§£üíÄ   \n",
       "...                                                   ...    ...   \n",
       "100032                                            ŸÖŸÖŸÜŸàŸÜŸÖ‚ù§      ‚ù§   \n",
       "100033                                               ‚ù§Ô∏è‚ù§Ô∏è   ‚ù§Ô∏è‚ù§Ô∏è   \n",
       "100035                                                ü§çü§çü§ç    ü§çü§çü§ç   \n",
       "100039                                                 ü•∞ü•µ     ü•∞ü•µ   \n",
       "100040               Stop watching Turkish soap opera.üçºü¶É      üçºü¶É   \n",
       "\n",
       "                                            SeparatedText EmojiSequences  \n",
       "2          Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ           [üíÄüíÄ]  \n",
       "11                        üò≠üò≠üò≠üò≠üò≠  ur ugly for that wallahi        [üò≠üò≠üò≠üò≠üò≠]  \n",
       "17               Hacer cosas de novios sin ser novios ‚ô• Ô∏è            [‚ô•]  \n",
       "18      ÿßŸÜÿß ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ŸÑÿß ŸÉŸÜ ÿ£ÿ±Ÿâ ÿ£ŸÜ ÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä ÿßŸÅÿ∂ŸÑ ŸÅŸä ÿßÿ≥Ÿàÿ° ÿ≠ÿß...            [üòÇ]  \n",
       "21                             Lmaooo i felt that shit ü§£üíÄ           [ü§£üíÄ]  \n",
       "...                                                   ...            ...  \n",
       "100032                                           ŸÖŸÖŸÜŸàŸÜŸÖ ‚ù§            [‚ù§]  \n",
       "100033                                            ‚ù§ Ô∏è ‚ù§ Ô∏è         [‚ù§, ‚ù§]  \n",
       "100035                                                ü§çü§çü§ç          [ü§çü§çü§ç]  \n",
       "100039                                                 ü•∞ü•µ           [ü•∞ü•µ]  \n",
       "100040               Stop watching Turkish soap opera. üçºü¶É           [üçºü¶É]  \n",
       "\n",
       "[20637 rows x 11 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to txt file (used for embeddings later)\n",
    "tweets_with_emojis.to_csv(\n",
    "    \"Data/2024-05-07-13-10/tweets_with_emojis.txt\",\n",
    "    columns=[\"SeparatedText\"],\n",
    "    header=False,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data to csv\n",
    "tweets_with_emojis.to_csv(\"Data/2024-05-07-13-10/tweets_with_emojis_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "data_dir = \"./Data/2024-05-07-13-10\"\n",
    "# Load data from CSV\n",
    "tweets_with_emojis = pd.read_csv(\n",
    "    f\"{data_dir}/tweets_with_emojis_dataframe.csv\", index_col=0\n",
    ")\n",
    "\n",
    "# Convert string representation of lists to actual lists\n",
    "tweets_with_emojis[\"EmojiSequences\"] = tweets_with_emojis[\"EmojiSequences\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 26M words\n",
      "Number of words:  796246\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  108542 lr:  0.000000 avg.loss:  1.053137 ETA:   0h 0m 0s 40.4% words/sec/thread:  112825 lr:  0.029813 avg.loss:  1.497900 ETA:   0h 1m41s\n",
      "Read 26M words\n",
      "Number of words:  796246\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  308815 lr:  0.000000 avg.loss:  1.347733 ETA:   0h 0m 0s  6.0% words/sec/thread:  302074 lr:  0.046991 avg.loss:  3.138034 ETA:   0h 0m59s1.347733 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "\n",
    "# Print test results\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "\n",
    "# Set data directory\n",
    "data_dir = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "data_file = os.path.join(data_dir, \"tweets_with_emojis.txt\")\n",
    "\n",
    "\n",
    "# SGNS model\n",
    "model = fasttext.train_unsupervised(\n",
    "    input=data_file,\n",
    "    model=\"skipgram\",\n",
    "    ws=4,\n",
    "    minCount=2,\n",
    "    minn=0,\n",
    "    maxn=0,\n",
    "    neg=5,\n",
    ")\n",
    "\n",
    "# Save bin format for SGNS\n",
    "model.save_model(os.path.join(data_dir, \"model_tweets_with_emojis_sgns.bin\"))\n",
    "\n",
    "\n",
    "# CBOW model\n",
    "model = fasttext.train_unsupervised(\n",
    "    input=data_file,\n",
    "    model=\"cbow\",\n",
    "    ws=4,\n",
    "    minCount=2,\n",
    "    minn=0,\n",
    "    maxn=0,\n",
    "    neg=5,\n",
    ")\n",
    "\n",
    "# Save bin format for CBOW\n",
    "model.save_model(os.path.join(data_dir, \"model_tweets_with_emojis_cbow.bin\"))\n",
    "\n",
    "\n",
    "# Convert SGNS from bin to vec format\n",
    "model = gensim.models.fasttext.load_facebook_vectors(\n",
    "    os.path.join(data_dir, \"model_tweets_with_emojis_sgns.bin\")\n",
    ")\n",
    "model.save_word2vec_format(os.path.join(data_dir, \"model_tweets_with_emojis_sgns.vec\"))\n",
    "\n",
    "\n",
    "# Convert CBOW from bin to vec format\n",
    "model = gensim.models.fasttext.load_facebook_vectors(\n",
    "    os.path.join(data_dir, \"model_tweets_with_emojis_cbow.bin\")\n",
    ")\n",
    "model.save_word2vec_format(os.path.join(data_dir, \"model_tweets_with_emojis_cbow.vec\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City Lang                 Time  \\\n",
      "2                tsiombe   fr  2021-12-06 15:50:41   \n",
      "11                 hobyo   en  2021-12-06 23:04:04   \n",
      "21        villa yapacani   en  2021-12-06 23:28:34   \n",
      "32          vangaindrano  und  2021-12-06 23:40:46   \n",
      "36         david-gorodok   en  2021-12-06 23:48:44   \n",
      "...                  ...  ...                  ...   \n",
      "12744226            soka   ja  2021-12-21 16:43:23   \n",
      "12744230        rumuruti  und  2021-12-21 16:43:24   \n",
      "12744299            soka   ja  2021-12-21 16:43:35   \n",
      "12744306  klosterneuburg   en  2021-12-21 16:43:36   \n",
      "12744323            soka   en  2021-12-21 16:43:38   \n",
      "\n",
      "                                                       Text       Geohash  \\\n",
      "2         Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ...  m5824frby6pt   \n",
      "11                  @M0zark0 üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi  t0fw2npnp7hn   \n",
      "21                  @andalechuey Lmaooo i felt that shit ü§£üíÄ  6sft52mvxsct   \n",
      "32                             üò≠üò≠üò≠üò≠ https://t.co/aUyQoey5Wc  m5cg5jm22ke2   \n",
      "36        Hello!  PaidTunes is a company that pays you w...  u972576t4r3b   \n",
      "...                                                     ...           ...   \n",
      "12744226                                 @yuyyumaki „Åä„Åä„Åäüò≥üíóüíóüíó  xn77v2fkpxu8   \n",
      "12744230                      @ingasiania @DanAlumasa üòÇüòÇüòÇüòÇüòÇ  sb1cx2dc2q72   \n",
      "12744299                                  @reopakame ÈóáÁ≥ª„Åô„Åçü§§üíï  xn77v2fkpxu8   \n",
      "12744306  @cricketisle You could try it out! But for me ...  u2edg3xwjppu   \n",
      "12744323  See you in the Solice metaverse https://t.co/H...  xn77v2fkpxu8   \n",
      "\n",
      "         Country         Region  \\\n",
      "2             mg     africa_sub   \n",
      "11            so   africa_north   \n",
      "21            bo  america_south   \n",
      "32            mg     africa_sub   \n",
      "36            by    europe_east   \n",
      "...          ...            ...   \n",
      "12744226      jp      asia_east   \n",
      "12744230      ke     africa_sub   \n",
      "12744299      jp      asia_east   \n",
      "12744306      at    europe_west   \n",
      "12744323      jp      asia_east   \n",
      "\n",
      "                                                CleanedText Emojis  \\\n",
      "2           Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ      üíÄüíÄ   \n",
      "11                           üò≠üò≠üò≠üò≠üò≠ ur ugly for that wallahi  üò≠üò≠üò≠üò≠üò≠   \n",
      "21                               Lmaooo i felt that shit ü§£üíÄ     ü§£üíÄ   \n",
      "32                                                    üò≠üò≠üò≠üò≠    üò≠üò≠üò≠üò≠   \n",
      "36        Hello!  PaidTunes is a company that pays you w...     üí∞üí∞   \n",
      "...                                                     ...    ...   \n",
      "12744226                                            „Åä„Åä„Åäüò≥üíóüíóüíó   üò≥üíóüíóüíó   \n",
      "12744230                                              üòÇüòÇüòÇüòÇüòÇ  üòÇüòÇüòÇüòÇüòÇ   \n",
      "12744299                                             ÈóáÁ≥ª„Åô„Åçü§§üíï     ü§§üíï   \n",
      "12744306   You could try it out! But for me it‚Äòs also th...     üòäüòâ   \n",
      "12744323  See you in the Solice metaverse  #soliceio #so...     üëçüëç   \n",
      "\n",
      "                                              SeparatedText EmojiSequences  \n",
      "2            Eh le poto il est √† l‚Äôaise avec sa question üíÄüíÄ           [üíÄüíÄ]  \n",
      "11                          üò≠üò≠üò≠üò≠üò≠  ur ugly for that wallahi        [üò≠üò≠üò≠üò≠üò≠]  \n",
      "21                               Lmaooo i felt that shit ü§£üíÄ           [ü§£üíÄ]  \n",
      "32                                                     üò≠üò≠üò≠üò≠         [üò≠üò≠üò≠üò≠]  \n",
      "36        Hello!  PaidTunes is a company that pays you w...           [üí∞üí∞]  \n",
      "...                                                     ...            ...  \n",
      "12744226                                           „Åä„Åä„Åä üò≥üíóüíóüíó         [üò≥üíóüíóüíó]  \n",
      "12744230                                              üòÇüòÇüòÇüòÇüòÇ        [üòÇüòÇüòÇüòÇüòÇ]  \n",
      "12744299                                            ÈóáÁ≥ª„Åô„Åç ü§§üíï           [ü§§üíï]  \n",
      "12744306  You could try it out! But for me it‚Äòs also tha...           [üòäüòâ]  \n",
      "12744323  See you in the Solice metaverse  #soliceio #so...           [üëçüëç]  \n",
      "\n",
      "[1107101 rows x 11 columns]\n",
      "FastTextKeyedVectors<vector_size=100, 796246 keys>\n",
      "[[ 0.41380402 -0.11446256 -0.7173146  ...  0.53497213  0.5812335\n",
      "   0.32401615]\n",
      " [ 0.45882672  0.37555623 -0.5633557  ...  0.10746752  0.18964337\n",
      "   0.48710862]\n",
      " [ 0.2746834   0.14251009 -0.21636261 ...  0.27197945  0.24982107\n",
      "  -0.19502364]\n",
      " ...\n",
      " [-0.12353732  0.12085461  0.13683707 ... -0.20110886  0.042679\n",
      "  -0.34974697]\n",
      " [ 0.28422418 -0.10544135 -0.40650862 ...  0.23608352  0.17467871\n",
      "  -0.5783039 ]\n",
      " [-0.34844565 -0.10982028  0.04284889 ...  0.6962104   0.78057766\n",
      "  -0.62641037]]\n",
      "       EmojiSequences Topic\n",
      "922344            ü§©ü§©ü§©     0\n",
      "216442             üòçü•∫     0\n",
      "707227             üòçüî•     0\n",
      "332597             üòçüî•     0\n",
      "22277             üëåüëåüëå     0\n",
      "...               ...   ...\n",
      "828018       ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£     9\n",
      "958233         ü§£ü§£ü§£ü§£ü§£ü§£     9\n",
      "339586          ü§£ü§£ü§£ü§£ü§£     9\n",
      "827995          ü§£ü§£ü§£ü§£ü§£     9\n",
      "387824        ü§£ü§£ü§£ü§£ü§£ü§£ü§£     9\n",
      "\n",
      "[990194 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import emoji\n",
    "\n",
    "figure(figsize=(12, 12), dpi=150)\n",
    "\n",
    "# Set file names and location\n",
    "root = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "sg_file = os.path.join(root, \"model_tweets_with_emojis_sgns.bin\")\n",
    "cbow_file = os.path.join(root, \"model_tweets_with_emojis_cbow.bin\")\n",
    "\n",
    "# Load vocab features (tweets )\n",
    "vocab_df = tweets_with_emojis[\n",
    "    tweets_with_emojis[\"EmojiSequences\"].apply(\n",
    "        lambda sequences: all(emoji.emoji_count(seq) > 1 for seq in sequences)\n",
    "    )\n",
    "]\n",
    "print(vocab_df)\n",
    "\n",
    "# Load embedding (change file to change type)\n",
    "model = gensim.models.fasttext.load_facebook_vectors(sg_file)\n",
    "model_type = \"cbow\"\n",
    "print(model)\n",
    "\n",
    "# Get embeddings for the vocab in the file\n",
    "embeddings = []\n",
    "emoji_sequences_with_embeddings = []\n",
    "for emoji_sequences_list in vocab_df.loc[:, \"EmojiSequences\"].values:\n",
    "    for emoji_sequence in emoji_sequences_list:\n",
    "        try:\n",
    "            embeddings.append(model[emoji_sequence])\n",
    "            emoji_sequences_with_embeddings.append(emoji_sequence)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "emoji_seq_with_embeddings_df = pd.DataFrame(\n",
    "    {\"EmojiSequences\": emoji_sequences_with_embeddings}\n",
    ")\n",
    "vocab_df = emoji_seq_with_embeddings_df\n",
    "\n",
    "\n",
    "# Now individual arrays into single matrix (note shifting types!)\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(embeddings)\n",
    "\n",
    "# Cluster words\n",
    "cluster = KMeans(n_clusters=20, init=\"k-means++\", max_iter=300)\n",
    "\n",
    "# Fit cluster (i.e., run it)\n",
    "cluster.fit(embeddings)\n",
    "\n",
    "# Now get cluster labels\n",
    "vocab_df.loc[:, \"Topic\"] = [str(x) for x in cluster.labels_]\n",
    "vocab_df.sort_values(\"Topic\", inplace=True)\n",
    "print(vocab_df)\n",
    "\n",
    "# Now save and visualize clusters\n",
    "root = os.path.join(\".\", \"Clusters/2024-05-08-13-20\")\n",
    "for cluster, cluster_df in vocab_df.groupby(\"Topic\"):\n",
    "    # Get vocab items in the model\n",
    "    vocab = set(\n",
    "        [\n",
    "            seq\n",
    "            for seq in cluster_df.loc[:, \"EmojiSequences\"].values\n",
    "            if not np.all(model[seq] == 0)  # Make sure embedding is not 0\n",
    "        ]\n",
    "    )\n",
    "    save = True\n",
    "\n",
    "    if vocab == []:\n",
    "        continue\n",
    "\n",
    "    # Now find the distance of each word from the centroid\n",
    "    cluster_shape = model.rank_by_centrality(vocab, use_norm=True)\n",
    "\n",
    "    cluster_shape = pd.DataFrame(cluster_shape, columns=[\"Similarity\", \"EmojiSequence\"])\n",
    "\n",
    "    # print(cluster_shape)\n",
    "    cluster_shape.to_csv(\n",
    "        os.path.join(root, \"Clusters.\" + model_type + \".\" + str(cluster) + \".csv\")\n",
    "    )\n",
    "\n",
    "    # Get distance from the center word\n",
    "    try:\n",
    "        distances_center = model.distances(\n",
    "            cluster_shape.iloc[0, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "    try:\n",
    "        distances_edge = model.distances(\n",
    "            cluster_shape.iloc[-1, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "\n",
    "    # Just to catch any key errors for missing words\n",
    "    if save == True:\n",
    "        # Make dataframe\n",
    "        cluster_graph = pd.DataFrame([vocab, distances_center, distances_edge]).T\n",
    "        cluster_graph.columns = [\"Word\", \"Distance from Center\", \"Distance from Edge\"]\n",
    "\n",
    "        # Make graph of the cluster\n",
    "        ax = sns.scatterplot(\n",
    "            data=cluster_graph, x=\"Distance from Center\", y=\"Distance from Edge\"\n",
    "        )\n",
    "\n",
    "        # Add words to label points\n",
    "        for row in cluster_graph.itertuples():\n",
    "            emoji_sequence = row[1]\n",
    "            x = row[2]\n",
    "            y = row[3]\n",
    "\n",
    "            # plt.annotate(\n",
    "            #     emoji.demojize(emoji_sequence).strip(\":\"),\n",
    "            #     (x, y),\n",
    "            #     textcoords=\"offset points\",\n",
    "            #     xytext=(0, 10),\n",
    "            #     ha=\"center\",\n",
    "            # )\n",
    "\n",
    "        # Save\n",
    "        plt.savefig(\n",
    "            os.path.join(root, \"Clusters.\" + model_type + \".\" + str(cluster) + \".png\"),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=150,\n",
    "        )\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with both individual emojis and sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextKeyedVectors<vector_size=100, 796246 keys>\n",
      "[[ 0.41380402 -0.11446256 -0.7173146  ...  0.53497213  0.5812335\n",
      "   0.32401615]\n",
      " [ 0.45882672  0.37555623 -0.5633557  ...  0.10746752  0.18964337\n",
      "   0.48710862]\n",
      " [ 0.23250706  0.81500727  1.166513   ...  0.3877129  -0.26803163\n",
      "  -1.1895267 ]\n",
      " ...\n",
      " [ 0.27002043 -1.1184332   0.01037823 ... -0.4062101  -0.22688912\n",
      "   0.53178984]\n",
      " [-1.138427   -0.09576458  0.51226735 ... -0.24816284  1.3126827\n",
      "  -1.0916843 ]\n",
      " [ 0.27747092  0.02784789  0.34089074 ... -0.24584675  0.31233457\n",
      "  -0.3711467 ]]\n",
      "        EmojiSequences Topic\n",
      "2074828              üëå     0\n",
      "1498618              üëå     0\n",
      "3397247              üëå     0\n",
      "2909770              üëå     0\n",
      "878794               üëå     0\n",
      "...                ...   ...\n",
      "3462411              üíõ     9\n",
      "2093404              üß°     9\n",
      "2902256              üíú     9\n",
      "3347091              üñ§     9\n",
      "1583919              üíú     9\n",
      "\n",
      "[3528540 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import emoji\n",
    "\n",
    "figure(figsize=(12, 12), dpi=150)\n",
    "\n",
    "# Set file names and location\n",
    "root = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "sg_file = os.path.join(root, \"model_tweets_with_emojis_sgns.bin\")\n",
    "cbow_file = os.path.join(root, \"model_tweets_with_emojis_cbow.bin\")\n",
    "\n",
    "# Load vocab features\n",
    "vocab_df = tweets_with_emojis\n",
    "\n",
    "# Load embedding (change file to change type)\n",
    "model = gensim.models.fasttext.load_facebook_vectors(sg_file)\n",
    "model_type = \"cbow\"\n",
    "print(model)\n",
    "\n",
    "# Get embeddings for the vocab in the file\n",
    "embeddings = []\n",
    "emoji_sequences_with_embeddings = []\n",
    "for emoji_sequences_list in vocab_df.loc[:, \"EmojiSequences\"].values:\n",
    "    for emoji_sequence in emoji_sequences_list:\n",
    "        try:\n",
    "            embeddings.append(model[emoji_sequence])\n",
    "            emoji_sequences_with_embeddings.append(emoji_sequence)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "emoji_seq_with_embeddings_df = pd.DataFrame(\n",
    "    {\"EmojiSequences\": emoji_sequences_with_embeddings}\n",
    ")\n",
    "vocab_df = emoji_seq_with_embeddings_df\n",
    "\n",
    "\n",
    "# Now individual arrays into single matrix (note shifting types!)\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(embeddings)\n",
    "\n",
    "# Cluster words\n",
    "cluster = KMeans(n_clusters=20, init=\"k-means++\", max_iter=300)\n",
    "\n",
    "# Fit cluster (i.e., run it)\n",
    "cluster.fit(embeddings)\n",
    "\n",
    "# Now get cluster labels\n",
    "vocab_df.loc[:, \"Topic\"] = [str(x) for x in cluster.labels_]\n",
    "vocab_df.sort_values(\"Topic\", inplace=True)\n",
    "print(vocab_df)\n",
    "\n",
    "# Now save and visualize clusters\n",
    "root = os.path.join(\".\", \"Clusters/2024-05-09-12-05\")\n",
    "for cluster, cluster_df in vocab_df.groupby(\"Topic\"):\n",
    "    # Get the vocab items that are in model\n",
    "    vocab = set(\n",
    "        [\n",
    "            seq\n",
    "            for seq in cluster_df.loc[:, \"EmojiSequences\"].values\n",
    "            if not np.all(model[seq] == 0)  # Make sure embedding is not 0\n",
    "        ]\n",
    "    )\n",
    "    save = True\n",
    "\n",
    "    if vocab == []:\n",
    "        continue\n",
    "\n",
    "    # Now find the distance of each word from the centroid\n",
    "    cluster_shape = model.rank_by_centrality(vocab, use_norm=True)\n",
    "\n",
    "    cluster_shape = pd.DataFrame(cluster_shape, columns=[\"Similarity\", \"EmojiSequence\"])\n",
    "\n",
    "    # print(cluster_shape)\n",
    "    cluster_shape.to_csv(\n",
    "        os.path.join(\n",
    "            root, \"Clusters.IndivAndSeq.\" + model_type + \".\" + str(cluster) + \".csv\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get distance from the center word\n",
    "    try:\n",
    "        distances_center = model.distances(\n",
    "            cluster_shape.iloc[0, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "    try:\n",
    "        distances_edge = model.distances(\n",
    "            cluster_shape.iloc[-1, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "\n",
    "    # Just to catch any key errors for missing words\n",
    "    if save == True:\n",
    "        # Make dataframe\n",
    "        cluster_graph = pd.DataFrame([vocab, distances_center, distances_edge]).T\n",
    "        cluster_graph.columns = [\"Word\", \"Distance from Center\", \"Distance from Edge\"]\n",
    "\n",
    "        # Make graph of the cluster\n",
    "        ax = sns.scatterplot(\n",
    "            data=cluster_graph, x=\"Distance from Center\", y=\"Distance from Edge\"\n",
    "        )\n",
    "\n",
    "        # Add words to label points\n",
    "        for row in cluster_graph.itertuples():\n",
    "            emoji_sequence = row[1]\n",
    "            x = row[2]\n",
    "            y = row[3]\n",
    "\n",
    "        # Save\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                root, \"Clusters.IndivAndSeq.\" + model_type + \".\" + str(cluster) + \".png\"\n",
    "            ),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=150,\n",
    "        )\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 1.905306602743449\n"
     ]
    }
   ],
   "source": [
    "# Flatten list of sequences\n",
    "all_sequences = [sequence for sequences in tweets_with_emojis['EmojiSequences'] for sequence in sequences]\n",
    "all_sequences_at_least_2 = [sequence for sequence in all_sequences if emoji.emoji_count(sequence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.339016947504073"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([emoji.emoji_count(sequence) for sequence in all_sequences_at_least_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([emoji.emoji_count(sequence) for sequence in all_sequences_at_least_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2992375"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tweets used?\n",
    "len(tweets_with_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244654"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique sequences of at least 2?\n",
    "len(set(all_sequences_at_least_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique individual emojis?\n",
    "all_emojis_from_data = sorted([''.join(get_emojis(x)) for x in list(set(''.join(tweets_with_emojis.loc[:,\"Emoji\"].values)))])\n",
    "all_unique_emojis_from_data = set(all_emojis_from_data)\n",
    "len(all_unique_emojis_from_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common sequences:\n",
      "üòÇüòÇüòÇ : 57871\n",
      "üòÇüòÇ : 56719\n",
      "ü§£ü§£ü§£ : 33592\n",
      "ü§£ü§£ : 24275\n",
      "üòÇüòÇüòÇüòÇ : 24073\n",
      "üò≠üò≠üò≠ : 20088\n",
      "üò≠üò≠ : 19998\n",
      "ü§£ü§£ü§£ü§£ : 15248\n",
      "üòÇüòÇüòÇüòÇüòÇ : 11023\n",
      "üò≠üò≠üò≠üò≠ : 8581\n",
      "10 most common emojis:\n",
      "‚ù§ : 183941\n",
      "üòÇ : 153454\n",
      "üò≠ : 68989\n",
      "ü§£ : 68818\n",
      "ü•∫ : 41917\n",
      "üòç : 40050\n",
      "üòÖ : 37134\n",
      "‚ô• : 36839\n",
      "üòä : 33679\n",
      "ü•∞ : 31201\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each sequence\n",
    "sequence_counter = Counter(all_sequences_at_least_2)\n",
    "most_common_sequences = sequence_counter.most_common(10)\n",
    "\n",
    "print(\"10 most common sequences:\")\n",
    "for sequence, count in most_common_sequences:\n",
    "    if emoji.emoji_count(sequence) >= 2:\n",
    "        print(sequence, \":\", count)\n",
    "\n",
    "\n",
    "# Count occurrences of each emoji\n",
    "sequence_counter = Counter([seq for seq in all_sequences if len(get_emojis(seq)) == 1])\n",
    "most_common_emojis = sequence_counter.most_common(10)\n",
    "\n",
    "print(\"10 most common emojis:\")\n",
    "for common_emoji, count in most_common_emojis:\n",
    "    print(common_emoji, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fasttext.FastText._FastText object at 0x279b882c0>\n",
      "getting distances for üôåüôÉ\n",
      "getting distances for üëè‚öΩ\n",
      "getting distances for üáµüáπüáµüáπüáµüáπ\n",
      "getting distances for üéÇüçæüå≤\n",
      "getting distances for üí™üèºü§úüèºü§õüèº\n",
      "getting distances for üòêüëç\n",
      "getting distances for üò≠üò≠üò¢üò¢\n",
      "getting distances for ü§™üëå\n",
      "getting distances for üë©üèæ‚Äç‚ù§\n",
      "getting distances for üò∑ü¶†üíâ\n",
      "getting distances for üòÇüòÇüòÇüíîüíîüíî\n",
      "getting distances for üëçü•∂\n",
      "getting distances for üòÇüòÇüò≠üò≠\"\n",
      "getting distances for üò¨ü§î\"\n",
      "getting distances for üçøüçøüçøüçøüçøüçøüçøüçø\n",
      "getting distances for ü•≤üíïüíï\n",
      "getting distances for üå≥üå∑\n",
      "getting distances for üê∂üí©\n",
      "getting distances for ü§©üëå\"\n",
      "getting distances for ü§£ü§£üôè\n",
      "       Similarity Emoji Sequence Target Emoji Sequence\n",
      "28784    1.000000             üôåüôÉ                    üôåüôÉ\n",
      "29581    0.934571            ‚ú®üôå\"                    üôåüôÉ\n",
      "54220    0.925216             üò≥üõê                    üôåüôÉ\n",
      "38791    0.919492            ü§óüëèüèº                    üôåüôÉ\n",
      "25509    0.918676            üòçüôå\"                    üôåüôÉ\n",
      "...           ...            ...                   ...\n",
      "44593    0.879122           ü§©üéâüéÅüíµ                   ü§£ü§£üôè\n",
      "24998    0.877754             üò©üöÆ                   ü§£ü§£üôè\n",
      "39954    0.877603             üòÇüêé                   ü§£ü§£üôè\n",
      "34584    0.877298           üì†üì†üì†üì†                   ü§£ü§£üôè\n",
      "41165    0.877155            ü•∫üôÑüòÇ                   ü§£ü§£üôè\n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fasttext.FastText._FastText object at 0x277abf3e0>\n",
      "getting distances for üôåüôÉ\n",
      "getting distances for üëè‚öΩ\n",
      "getting distances for üáµüáπüáµüáπüáµüáπ\n",
      "getting distances for üéÇüçæüå≤\n",
      "getting distances for üí™üèºü§úüèºü§õüèº\n",
      "getting distances for üòêüëç\n",
      "getting distances for üò≠üò≠üò¢üò¢\n",
      "getting distances for ü§™üëå\n",
      "getting distances for üë©üèæ‚Äç‚ù§\n",
      "getting distances for üò∑ü¶†üíâ\n",
      "getting distances for üòÇüòÇüòÇüíîüíîüíî\n",
      "getting distances for üëçü•∂\n",
      "getting distances for üòÇüòÇüò≠üò≠\"\n",
      "getting distances for üò¨ü§î\"\n",
      "getting distances for üçøüçøüçøüçøüçøüçøüçøüçø\n",
      "getting distances for ü•≤üíïüíï\n",
      "getting distances for üå≥üå∑\n",
      "getting distances for üê∂üí©\n",
      "getting distances for ü§©üëå\"\n",
      "getting distances for ü§£ü§£üôè\n",
      "       Similarity Emoji Sequence Target Emoji Sequence\n",
      "28784    1.000000             üôåüôÉ                    üôåüôÉ\n",
      "6854     0.863498             ‚ù§üé∂                    üôåüôÉ\n",
      "6872     0.859594            ü•öü•öü•ö                    üôåüôÉ\n",
      "13799    0.859109            ü§óüòç\"                    üôåüôÉ\n",
      "10228    0.850270             üòàü§§                    üôåüôÉ\n",
      "...           ...            ...                   ...\n",
      "19272    0.856314            üôèüèºüçÄ                   ü§£ü§£üôè\n",
      "10822    0.854694            üò≠üòî\"                   ü§£ü§£üôè\n",
      "11982    0.854626            ü§óü§óü•∞                   ü§£ü§£üôè\n",
      "6178     0.853978             üíØü§ù                   ü§£ü§£üôè\n",
      "4159     0.853913             ü•∞üò≠                   ü§£ü§£üôè\n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(12, 6), dpi=150)\n",
    "\n",
    "# Get nearest neighbors for target word (i.e., target emoji)\n",
    "def get_distances(model, target, vocab):\n",
    "    # Get embedding for target emoji\n",
    "    target_embedding = model[target].reshape(1, -1)\n",
    "\n",
    "    # Get cosine similarity for each word in vocab\n",
    "    distances = []\n",
    "    for word in vocab:\n",
    "        similarity = cosine_similarity(target_embedding, model[word].reshape(1, -1))[0][0]\n",
    "        distances.append([similarity, word])\n",
    "\n",
    "    # Make dataframe\n",
    "    results_df = pd.DataFrame(distances, columns=[\"Similarity\", \"Emoji Sequence\"])\n",
    "    results_df.sort_values(\"Similarity\", ascending=False, inplace=True)\n",
    "    results_df = results_df.head(20)\n",
    "    results_df.loc[:,\"Target Emoji Sequence\"] = target\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Set file names and location\n",
    "data_dir = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "sg_file = os.path.join(data_dir, \"model_tweets_with_emojis_sgns.bin\")\n",
    "cbow_file = os.path.join(data_dir, \"model_tweets_with_emojis_cbow.bin\")\n",
    "\n",
    "\n",
    "# Get 20 random emoji sequences\n",
    "# words = random.sample(tweets_with_emojis.loc[:,\"Emojis\"], 20)\n",
    "# words = get_emoji_sample_with_categories(n=20) \n",
    "seq_in_model = [w for w in model.get_words() if emoji.emoji_count(w) > 1]\n",
    "words = random.sample(seq_in_model, 20)\n",
    "\n",
    "# Generate id for file\n",
    "tag = str(random.randint(1,1000))\n",
    "\n",
    "# For each type of embedding\n",
    "for embedding_file in [sg_file, cbow_file]:\n",
    "    # Get model_type\n",
    "    if \"sgns\" in embedding_file:\n",
    "        model_type = \"sg\"\n",
    "    elif \"cbow\" in embedding_file:\n",
    "        model_type = \"cbow\"\n",
    "\n",
    "    # Load embedding \n",
    "    model = fasttext.load_model(embedding_file)\n",
    "    print(model)\n",
    "\n",
    "    # Iterate over words (emojis from sample)\n",
    "    stack = []\n",
    "    for emoji_sequence in words:\n",
    "        print(f\"getting distances for {emoji_sequence}\")\n",
    "        # emojis_in_same_category = all_emojis_by_category[all_emojis_by_category['Category'] == the_emoji_category]\n",
    "        nearest = get_distances(model, emoji_sequence, seq_in_model)\n",
    "        stack.append(nearest)\n",
    "\n",
    "    # Concat\n",
    "    nearest_df = pd.concat(stack)\n",
    "    print(nearest_df)\n",
    "    \n",
    "    # Save\n",
    "    file = f\"Neighbors/2024-05-08-07-10/TweetsWithEmojis.Neighbors.{tag}.{model_type}\"\n",
    "    nearest_df.to_csv(file+\".csv\")\n",
    "    \n",
    "    # Graph\n",
    "    ax = sns.stripplot(data=nearest_df, x=\"Target Emoji Sequence\", hue=\"Target Emoji Sequence\", y=\"Similarity\", jitter=True, size=2)\n",
    "\n",
    "    # Hide word labels\n",
    "    frame = plt.gca()\n",
    "    # frame.axes.xaxis.set_ticklabels([f\"{emoji.demojize(w).strip(\":\")} {w}\" for w in words['Emoji']])\n",
    "    frame.axes.xaxis.set_ticklabels([])\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "    plt.savefig(file+\".neighborhoods.png\", dpi = 150, bbox_inches = \"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247621"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.load_model(cbow_file)\n",
    "# get_distances(model, ü•±ü§¢,üê∂üê∂üòä)\n",
    "word = \"ü•±ü§¢\"\n",
    "target = \"üê∂üê∂üòä\"\n",
    "target_embedding = model[target].reshape(1, -1)\n",
    "cosine_similarity(target_embedding, model[word].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60231"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([w for w in model.get_words() if emoji.emoji_count(w) > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ling413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
