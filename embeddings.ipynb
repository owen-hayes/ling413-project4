{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geohash</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pruzhany</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 15:13:17</td>\n",
       "      <td>@WarcraftDevs - demon spikes reflect x % of in...</td>\n",
       "      <td>u936uxwkybu2</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lumphat</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:25:06</td>\n",
       "      <td>@phuonganhh21 e</td>\n",
       "      <td>w6khf3wghq2s</td>\n",
       "      <td>kh</td>\n",
       "      <td>asia_southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsiombe</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-12-06 15:50:41</td>\n",
       "      <td>Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...</td>\n",
       "      <td>m5824frby6pt</td>\n",
       "      <td>mg</td>\n",
       "      <td>africa_sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boruny</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:55:03</td>\n",
       "      <td>@SobolLubov #ĞÑĞºĞ°Ñ€ĞŸÑƒÑ‚Ğ¸Ğ½Ñƒ #ĞÑĞºĞ°Ñ€ #ĞŸÑƒÑ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¼ #ĞĞ°Ğ²...</td>\n",
       "      <td>u9dt48j9wb99</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vilkaviskis</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 17:15:42</td>\n",
       "      <td>Top rated project! @polygen_io and #Launchpad ...</td>\n",
       "      <td>u98quj5919n4</td>\n",
       "      <td>lt</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744328</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>https://t.co/cDu892Apj6</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744329</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>ä»Šæ—¥ã ã‘ã§4ç‰é£Ÿã¹ã¦å†·å‡åº«ã‚’ç©ºã«ã—ãŸã€‚å¼•è¶Šã—å‰ã†ã©ã‚“ã€‚ https://t.co/zG57D...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744330</th>\n",
       "      <td>pedro betancourt</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>æ˜¯æˆ‘äº† https://t.co/KPTyf0hJCO</td>\n",
       "      <td>dhn1q1q7ttq1</td>\n",
       "      <td>cu</td>\n",
       "      <td>america_central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744331</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>ğŸ™‡â€â™€ï¸ğŸ’•ğŸ’• https://t.co/orOVDvtL85</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744332</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>@iiyohei ã¾ãšã¯é–¢æ±ï¼‘éƒ¨ã«æ˜‡æ ¼ã—ã¦è¿ãˆãŸã„ã§ã™ã­ãƒ¼ 12.25.11:00ã‹ã‚‰å…¥æ›¿ãˆ...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12744333 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      City Lang                 Time  \\\n",
       "0                 pruzhany   en  2021-12-06 15:13:17   \n",
       "1                  lumphat  und  2021-12-06 15:25:06   \n",
       "2                  tsiombe   fr  2021-12-06 15:50:41   \n",
       "3                   boruny  und  2021-12-06 15:55:03   \n",
       "4              vilkaviskis   en  2021-12-06 17:15:42   \n",
       "...                    ...  ...                  ...   \n",
       "12744328              soka  und  2021-12-21 16:43:39   \n",
       "12744329              soka   ja  2021-12-21 16:43:39   \n",
       "12744330  pedro betancourt   ja  2021-12-21 16:43:39   \n",
       "12744331              soka  und  2021-12-21 16:43:40   \n",
       "12744332              soka   ja  2021-12-21 16:43:40   \n",
       "\n",
       "                                                       Text       Geohash  \\\n",
       "0         @WarcraftDevs - demon spikes reflect x % of in...  u936uxwkybu2   \n",
       "1                                           @phuonganhh21 e  w6khf3wghq2s   \n",
       "2         Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...  m5824frby6pt   \n",
       "3         @SobolLubov #ĞÑĞºĞ°Ñ€ĞŸÑƒÑ‚Ğ¸Ğ½Ñƒ #ĞÑĞºĞ°Ñ€ #ĞŸÑƒÑ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¼ #ĞĞ°Ğ²...  u9dt48j9wb99   \n",
       "4         Top rated project! @polygen_io and #Launchpad ...  u98quj5919n4   \n",
       "...                                                     ...           ...   \n",
       "12744328                            https://t.co/cDu892Apj6  xn77v2fkpxu8   \n",
       "12744329  ä»Šæ—¥ã ã‘ã§4ç‰é£Ÿã¹ã¦å†·å‡åº«ã‚’ç©ºã«ã—ãŸã€‚å¼•è¶Šã—å‰ã†ã©ã‚“ã€‚ https://t.co/zG57D...  xn77v2fkpxu8   \n",
       "12744330                        æ˜¯æˆ‘äº† https://t.co/KPTyf0hJCO  dhn1q1q7ttq1   \n",
       "12744331                     ğŸ™‡â€â™€ï¸ğŸ’•ğŸ’• https://t.co/orOVDvtL85  xn77v2fkpxu8   \n",
       "12744332  @iiyohei ã¾ãšã¯é–¢æ±ï¼‘éƒ¨ã«æ˜‡æ ¼ã—ã¦è¿ãˆãŸã„ã§ã™ã­ãƒ¼ 12.25.11:00ã‹ã‚‰å…¥æ›¿ãˆ...  xn77v2fkpxu8   \n",
       "\n",
       "         Country           Region  \n",
       "0             by      europe_east  \n",
       "1             kh   asia_southeast  \n",
       "2             mg       africa_sub  \n",
       "3             by      europe_east  \n",
       "4             lt      europe_east  \n",
       "...          ...              ...  \n",
       "12744328      jp        asia_east  \n",
       "12744329      jp        asia_east  \n",
       "12744330      cu  america_central  \n",
       "12744331      jp        asia_east  \n",
       "12744332      jp        asia_east  \n",
       "\n",
       "[12744333 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tweets into a DataFrame\n",
    "tweets_df = pd.read_csv(\"../project2/Data.Original.1900\")\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geohash</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pruzhany</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 15:13:17</td>\n",
       "      <td>@WarcraftDevs - demon spikes reflect x % of in...</td>\n",
       "      <td>u936uxwkybu2</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lumphat</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:25:06</td>\n",
       "      <td>@phuonganhh21 e</td>\n",
       "      <td>w6khf3wghq2s</td>\n",
       "      <td>kh</td>\n",
       "      <td>asia_southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsiombe</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-12-06 15:50:41</td>\n",
       "      <td>Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...</td>\n",
       "      <td>m5824frby6pt</td>\n",
       "      <td>mg</td>\n",
       "      <td>africa_sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boruny</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-06 15:55:03</td>\n",
       "      <td>@SobolLubov #ĞÑĞºĞ°Ñ€ĞŸÑƒÑ‚Ğ¸Ğ½Ñƒ #ĞÑĞºĞ°Ñ€ #ĞŸÑƒÑ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¼ #ĞĞ°Ğ²...</td>\n",
       "      <td>u9dt48j9wb99</td>\n",
       "      <td>by</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vilkaviskis</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 17:15:42</td>\n",
       "      <td>Top rated project! @polygen_io and #Launchpad ...</td>\n",
       "      <td>u98quj5919n4</td>\n",
       "      <td>lt</td>\n",
       "      <td>europe_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744328</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>https://t.co/cDu892Apj6</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744329</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>ä»Šæ—¥ã ã‘ã§4ç‰é£Ÿã¹ã¦å†·å‡åº«ã‚’ç©ºã«ã—ãŸã€‚å¼•è¶Šã—å‰ã†ã©ã‚“ã€‚ https://t.co/zG57D...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744330</th>\n",
       "      <td>pedro betancourt</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:39</td>\n",
       "      <td>æ˜¯æˆ‘äº† https://t.co/KPTyf0hJCO</td>\n",
       "      <td>dhn1q1q7ttq1</td>\n",
       "      <td>cu</td>\n",
       "      <td>america_central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744331</th>\n",
       "      <td>soka</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>ğŸ™‡â€â™€ï¸ğŸ’•ğŸ’• https://t.co/orOVDvtL85</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744332</th>\n",
       "      <td>soka</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-21 16:43:40</td>\n",
       "      <td>@iiyohei ã¾ãšã¯é–¢æ±ï¼‘éƒ¨ã«æ˜‡æ ¼ã—ã¦è¿ãˆãŸã„ã§ã™ã­ãƒ¼ 12.25.11:00ã‹ã‚‰å…¥æ›¿ãˆ...</td>\n",
       "      <td>xn77v2fkpxu8</td>\n",
       "      <td>jp</td>\n",
       "      <td>asia_east</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12739981 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      City Lang                 Time  \\\n",
       "0                 pruzhany   en  2021-12-06 15:13:17   \n",
       "1                  lumphat  und  2021-12-06 15:25:06   \n",
       "2                  tsiombe   fr  2021-12-06 15:50:41   \n",
       "3                   boruny  und  2021-12-06 15:55:03   \n",
       "4              vilkaviskis   en  2021-12-06 17:15:42   \n",
       "...                    ...  ...                  ...   \n",
       "12744328              soka  und  2021-12-21 16:43:39   \n",
       "12744329              soka   ja  2021-12-21 16:43:39   \n",
       "12744330  pedro betancourt   ja  2021-12-21 16:43:39   \n",
       "12744331              soka  und  2021-12-21 16:43:40   \n",
       "12744332              soka   ja  2021-12-21 16:43:40   \n",
       "\n",
       "                                                       Text       Geohash  \\\n",
       "0         @WarcraftDevs - demon spikes reflect x % of in...  u936uxwkybu2   \n",
       "1                                           @phuonganhh21 e  w6khf3wghq2s   \n",
       "2         Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...  m5824frby6pt   \n",
       "3         @SobolLubov #ĞÑĞºĞ°Ñ€ĞŸÑƒÑ‚Ğ¸Ğ½Ñƒ #ĞÑĞºĞ°Ñ€ #ĞŸÑƒÑ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¼ #ĞĞ°Ğ²...  u9dt48j9wb99   \n",
       "4         Top rated project! @polygen_io and #Launchpad ...  u98quj5919n4   \n",
       "...                                                     ...           ...   \n",
       "12744328                            https://t.co/cDu892Apj6  xn77v2fkpxu8   \n",
       "12744329  ä»Šæ—¥ã ã‘ã§4ç‰é£Ÿã¹ã¦å†·å‡åº«ã‚’ç©ºã«ã—ãŸã€‚å¼•è¶Šã—å‰ã†ã©ã‚“ã€‚ https://t.co/zG57D...  xn77v2fkpxu8   \n",
       "12744330                        æ˜¯æˆ‘äº† https://t.co/KPTyf0hJCO  dhn1q1q7ttq1   \n",
       "12744331                     ğŸ™‡â€â™€ï¸ğŸ’•ğŸ’• https://t.co/orOVDvtL85  xn77v2fkpxu8   \n",
       "12744332  @iiyohei ã¾ãšã¯é–¢æ±ï¼‘éƒ¨ã«æ˜‡æ ¼ã—ã¦è¿ãˆãŸã„ã§ã™ã­ãƒ¼ 12.25.11:00ã‹ã‚‰å…¥æ›¿ãˆ...  xn77v2fkpxu8   \n",
       "\n",
       "         Country           Region  \n",
       "0             by      europe_east  \n",
       "1             kh   asia_southeast  \n",
       "2             mg       africa_sub  \n",
       "3             by      europe_east  \n",
       "4             lt      europe_east  \n",
       "...          ...              ...  \n",
       "12744328      jp        asia_east  \n",
       "12744329      jp        asia_east  \n",
       "12744330      cu  america_central  \n",
       "12744331      jp        asia_east  \n",
       "12744332      jp        asia_east  \n",
       "\n",
       "[12739981 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop empty rows\n",
    "tweets_df = tweets_df.dropna()\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_10000 = tweets_df[:10000]\n",
    "tweets_df_100k = tweets_df[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_emojis(text):\n",
    "    emojis = emoji.analyze(text)\n",
    "    return list(x[0] for x in emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider tweets which contain at least 1 emoji\n",
    "tweets_with_emojis = tweets_df[\n",
    "    tweets_df[\"Text\"].apply(lambda tweet_text: emoji.emoji_count(tweet_text) > 0)\n",
    "]\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column to generate 'CleanedText' column\n",
    "tweets_with_emojis[\"CleanedText\"] = tweets_with_emojis[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Get just emojis and store in 'Emojis' column\n",
    "tweets_with_emojis[\"Emojis\"] = tweets_with_emojis[\"Text\"].apply(\n",
    "    lambda text: \"\".join(get_emojis(text))\n",
    ")\n",
    "\n",
    "tweets_with_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€ \n",
       "11                        ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi\n",
       "17               Hacer cosas de novios sin ser novios â™¥ï¸\n",
       "18      Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§ÙØ¶Ù„ ÙÙŠ Ø§Ø³ÙˆØ¡ Ø­...\n",
       "21                            Lmaooo i felt that shit ğŸ¤£ğŸ’€\n",
       "29                                            ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ğŸ•º\n",
       "30                                                   â¤ï¸ \n",
       "32                                                 ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ \n",
       "36     Hello!  PaidTunes is a company that pays you w...\n",
       "42                                               Ù„ØµÙ‚ÙˆÙˆ ğŸ’™\n",
       "43                                       ØµÙ„ÙŠ Ø¹Ù„ÙŠ Ø§Ù„Ù†Ø¨ÙŠ ğŸ¤\n",
       "51     Ğ—Ğ½Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚Ñ‹Ğ¹ Ğ¸Ğ¼Ğ±Ğ¸Ñ€Ğ½Ñ‹Ğ¹ ÑĞ»ÑŒ ğŸº Ğ¸Ğ· ĞĞ½Ğ³Ğ»Ğ¸Ğ¸ ğŸ‡¬ğŸ‡§ ÑĞ»Ğ°Ğ´ĞºĞ¸Ğ¹...\n",
       "52     Ğ”Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ° Ñ…Ğ¾Ñ‡ĞµÑ‚ÑÑ Ğ¸Ğ·Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¸Ğ²Ğ¾ ğŸº Ğ¾Ñ‚ ÑÑ‚Ğ¾...\n",
       "55        Me casually cleaning all my guns and bullets ğŸ˜\n",
       "57                                        D-24 êµ¿ëª¨ë‹ ì‹¸ë‘í•´ğŸ’™ \n",
       "77                                3 shifts left at unm ğŸ¤©\n",
       "78      Yeah , we found out who lmfaoo but it threw u...\n",
       "83                       Me quede dormido en el bondiğŸ˜ğŸ˜ğŸ˜\n",
       "87                                           combinado ğŸ¤\n",
       "96                                   fuckin goodmorningğŸ¥°\n",
       "108                                        Np babes â¤ï¸â¤ï¸\n",
       "114         nossa fui fdp e jÃ¡ sei q vem karma por aÃ­ ğŸ˜–ğŸ˜–\n",
       "120              acho que encontrei meu antidepressivoğŸªğŸ¤\n",
       "123    Amo como ella me hace aterrizar, me enseÃ±a y m...\n",
       "126                                                  ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
       "127                                                    ğŸ¥·\n",
       "131                                        Dooooooo itğŸ‘€ \n",
       "132                                                  â­ğŸŒŠ \n",
       "133                                                   ğŸ”¥ \n",
       "135    ğŸ‘‡nunca hay que justificar a un violento, ni el...\n",
       "140                                                ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n",
       "147               Actually, I need to earn not to learnğŸ’”\n",
       "151    bakpelah guess ada sales ğŸ¥²ğŸ¥² kekgi nak g jalan ...\n",
       "152    alhamdulillah dapat insentif untuk 3bulan skli...\n",
       "160             Ğ½ÑĞ³ Ğ¼ÑƒÑƒ Ğ°Ğ¼Ğ±Ğ°Ğ°Ñ€ÑˆĞ´ÑÑ ÑÑƒĞ³ Ğ½ÑŒ ÑˆĞ°Ğ»Ğ³Ğ°Ğ´Ğ°Ğ³ ÑĞ¼ ğŸ¤£ \n",
       "161    Mis amigas siempre me preguntan porque estoy t...\n",
       "163      ğŸ˜´ was to easy cooz u should run it back and ...\n",
       "169                                ÙŠØ²ÙÙˆÙ†ÙŠ ÙˆØ§Ù†Ø§ Ù…Ø§Ù„ÙŠ Ø´ØºÙ„ğŸ¥²\n",
       "173                    Ø§Ù„Ù„Ù‡Ù… Ø§Ø¬Ø¹Ù„Ù†Ø§ Ù…Ù† Ø£ØµØ­Ø§Ø¨ Ø§Ù„ÙŠÙ…ÙŠÙ† ğŸ™â™¥ï¸ \n",
       "192           Water happy actor ğŸ˜„ğŸ˜„ @ Hilton Nay Pyi Taw \n",
       "206    ğŸ¡ Maison Ã  louer R+2  CitÃ© ImmobiliÃ¨re Tamatav...\n",
       "225                                            ã†ã¾ã™ãã§ã™ã­ğŸ˜³ğŸ˜³\n",
       "226        The world is a beautiful place ğŸ—º #ØµØ¨Ø§Ø­_Ø§Ù„Ø®ÙŠØ± \n",
       "227    Chufangagri #agricultural #drone spraying oper...\n",
       "228    ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ø¨ÙˆØ§Ø³Ø·Ø©SNAPTUBE. ...\n",
       "235    IDO Whitelist is open: ğŸ”—  ğŸ”—  Gadget War is a P...\n",
       "252                                     Follow my tips ğŸ˜€\n",
       "266                                   ğŸ˜‚ğŸ˜‚ ÛŒØ¹Ù†ÛŒ Ø´Ø§Ù†Ø³ØªÙˆ ...\n",
       "269     Doma laba, bet Å¡eit esi samiksÄ“jis 2 grupÄ“jum...\n",
       "270                         Kukumbigwa mari yeAirtimeğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
       "Name: CleanedText, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_emojis[:50]['CleanedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_regional_indicator_or_zwj_or_gender(char):\n",
    "    return char.encode(\"utf-8\")[:3] == b\"\\xf0\\x9f\\x87\" or char in [\n",
    "        \"\\u200d\",\n",
    "        \"\\u2640\",\n",
    "        \"\\u2642\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_emoji_sequences(text):\n",
    "    emoji_sequences = []\n",
    "    formatted_text = \"\"\n",
    "    current_sequence = \"\"\n",
    "    for i, char in enumerate(text):\n",
    "        # Check if is emoji or is regional indicator like \"ğŸ‡¦\"\n",
    "        if emoji.is_emoji(char) or is_regional_indicator_or_zwj_or_gender(char):\n",
    "            current_sequence += char\n",
    "        else:\n",
    "            if current_sequence:\n",
    "                emoji_sequences.append(current_sequence)\n",
    "                if formatted_text and formatted_text[-1] != \" \":\n",
    "                    # print(\"adding space\")\n",
    "                    formatted_text += \" \"\n",
    "                formatted_text += current_sequence\n",
    "                current_sequence = \"\"\n",
    "\n",
    "            if (\n",
    "                formatted_text\n",
    "                and formatted_text[-1] != \" \"\n",
    "                and (\n",
    "                    emoji.is_emoji(formatted_text[-1])\n",
    "                    or is_regional_indicator_or_zwj_or_gender(char)\n",
    "                )\n",
    "            ):\n",
    "                formatted_text += \" \"\n",
    "            formatted_text += char\n",
    "\n",
    "    if current_sequence:\n",
    "        emoji_sequences.append(current_sequence)\n",
    "        if formatted_text and formatted_text[-1] != \" \":\n",
    "            formatted_text += \" \"\n",
    "        formatted_text += current_sequence\n",
    "\n",
    "    return formatted_text.strip(), emoji_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences\n",
    "tweets_with_emojis[\"SeparatedText\"], tweets_with_emojis[\"EmojiSequences\"] = zip(\n",
    "    *tweets_with_emojis[\"CleanedText\"].apply(extract_emoji_sequences)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geohash</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>SeparatedText</th>\n",
       "      <th>EmojiSequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsiombe</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-12-06 15:50:41</td>\n",
       "      <td>Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...</td>\n",
       "      <td>m5824frby6pt</td>\n",
       "      <td>mg</td>\n",
       "      <td>africa_sub</td>\n",
       "      <td>Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€</td>\n",
       "      <td>ğŸ’€ğŸ’€</td>\n",
       "      <td>Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€</td>\n",
       "      <td>[ğŸ’€ğŸ’€]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hobyo</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 23:04:04</td>\n",
       "      <td>@M0zark0 ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi</td>\n",
       "      <td>t0fw2npnp7hn</td>\n",
       "      <td>so</td>\n",
       "      <td>africa_north</td>\n",
       "      <td>ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi</td>\n",
       "      <td>ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­</td>\n",
       "      <td>ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­  ur ugly for that wallahi</td>\n",
       "      <td>[ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ayny</td>\n",
       "      <td>es</td>\n",
       "      <td>2021-12-06 23:19:23</td>\n",
       "      <td>Hacer cosas de novios sin ser novios â™¥ï¸</td>\n",
       "      <td>tx08psjenf21</td>\n",
       "      <td>tj</td>\n",
       "      <td>asia_central</td>\n",
       "      <td>Hacer cosas de novios sin ser novios â™¥ï¸</td>\n",
       "      <td>â™¥ï¸</td>\n",
       "      <td>Hacer cosas de novios sin ser novios â™¥ ï¸</td>\n",
       "      <td>[â™¥]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aflou</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-12-06 23:23:17</td>\n",
       "      <td>@beINSPORTS_news Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±...</td>\n",
       "      <td>sn16pc6ddver</td>\n",
       "      <td>dz</td>\n",
       "      <td>africa_north</td>\n",
       "      <td>Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§ÙØ¶Ù„ ÙÙŠ Ø§Ø³ÙˆØ¡ Ø­...</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§ÙØ¶Ù„ ÙÙŠ Ø§Ø³ÙˆØ¡ Ø­Ø§...</td>\n",
       "      <td>[ğŸ˜‚]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>villa yapacani</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-06 23:28:34</td>\n",
       "      <td>@andalechuey Lmaooo i felt that shit ğŸ¤£ğŸ’€</td>\n",
       "      <td>6sft52mvxsct</td>\n",
       "      <td>bo</td>\n",
       "      <td>america_south</td>\n",
       "      <td>Lmaooo i felt that shit ğŸ¤£ğŸ’€</td>\n",
       "      <td>ğŸ¤£ğŸ’€</td>\n",
       "      <td>Lmaooo i felt that shit ğŸ¤£ğŸ’€</td>\n",
       "      <td>[ğŸ¤£ğŸ’€]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100032</th>\n",
       "      <td>bojnurd</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-12-15 10:53:44</td>\n",
       "      <td>@heyfa64 Ù…Ù…Ù†ÙˆÙ†Ù…â¤</td>\n",
       "      <td>tq8v0qw6k277</td>\n",
       "      <td>ir</td>\n",
       "      <td>asia_central</td>\n",
       "      <td>Ù…Ù…Ù†ÙˆÙ†Ù…â¤</td>\n",
       "      <td>â¤</td>\n",
       "      <td>Ù…Ù…Ù†ÙˆÙ†Ù… â¤</td>\n",
       "      <td>[â¤]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100033</th>\n",
       "      <td>kaliganj</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-15 10:53:44</td>\n",
       "      <td>@Milon10941303 â¤ï¸â¤ï¸</td>\n",
       "      <td>tupmhqbgvphc</td>\n",
       "      <td>bd</td>\n",
       "      <td>asia_south</td>\n",
       "      <td>â¤ï¸â¤ï¸</td>\n",
       "      <td>â¤ï¸â¤ï¸</td>\n",
       "      <td>â¤ ï¸ â¤ ï¸</td>\n",
       "      <td>[â¤, â¤]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100035</th>\n",
       "      <td>tiszaujvaros</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-15 10:53:44</td>\n",
       "      <td>@kyloliqq ğŸ¤ğŸ¤ğŸ¤</td>\n",
       "      <td>u2wbxvbrvkcq</td>\n",
       "      <td>hu</td>\n",
       "      <td>europe_east</td>\n",
       "      <td>ğŸ¤ğŸ¤ğŸ¤</td>\n",
       "      <td>ğŸ¤ğŸ¤ğŸ¤</td>\n",
       "      <td>ğŸ¤ğŸ¤ğŸ¤</td>\n",
       "      <td>[ğŸ¤ğŸ¤ğŸ¤]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100039</th>\n",
       "      <td>kuusamo</td>\n",
       "      <td>und</td>\n",
       "      <td>2021-12-15 10:53:45</td>\n",
       "      <td>@kiyowyd ğŸ¥°ğŸ¥µ</td>\n",
       "      <td>uesz20fesvvy</td>\n",
       "      <td>fi</td>\n",
       "      <td>europe_west</td>\n",
       "      <td>ğŸ¥°ğŸ¥µ</td>\n",
       "      <td>ğŸ¥°ğŸ¥µ</td>\n",
       "      <td>ğŸ¥°ğŸ¥µ</td>\n",
       "      <td>[ğŸ¥°ğŸ¥µ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100040</th>\n",
       "      <td>preveza</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-15 10:53:46</td>\n",
       "      <td>@alperaltunoz @Turukku_ @FilipposBetsan1 @Akde...</td>\n",
       "      <td>sqyv84tsnn32</td>\n",
       "      <td>gr</td>\n",
       "      <td>europe_west</td>\n",
       "      <td>Stop watching Turkish soap opera.ğŸ¼ğŸ¦ƒ</td>\n",
       "      <td>ğŸ¼ğŸ¦ƒ</td>\n",
       "      <td>Stop watching Turkish soap opera. ğŸ¼ğŸ¦ƒ</td>\n",
       "      <td>[ğŸ¼ğŸ¦ƒ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20637 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  City Lang                 Time  \\\n",
       "2              tsiombe   fr  2021-12-06 15:50:41   \n",
       "11               hobyo   en  2021-12-06 23:04:04   \n",
       "17                ayny   es  2021-12-06 23:19:23   \n",
       "18               aflou   ar  2021-12-06 23:23:17   \n",
       "21      villa yapacani   en  2021-12-06 23:28:34   \n",
       "...                ...  ...                  ...   \n",
       "100032         bojnurd   ar  2021-12-15 10:53:44   \n",
       "100033        kaliganj  und  2021-12-15 10:53:44   \n",
       "100035    tiszaujvaros  und  2021-12-15 10:53:44   \n",
       "100039         kuusamo  und  2021-12-15 10:53:45   \n",
       "100040         preveza   en  2021-12-15 10:53:46   \n",
       "\n",
       "                                                     Text       Geohash  \\\n",
       "2       Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...  m5824frby6pt   \n",
       "11                @M0zark0 ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi  t0fw2npnp7hn   \n",
       "17                Hacer cosas de novios sin ser novios â™¥ï¸  tx08psjenf21   \n",
       "18      @beINSPORTS_news Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±...  sn16pc6ddver   \n",
       "21                @andalechuey Lmaooo i felt that shit ğŸ¤£ğŸ’€  6sft52mvxsct   \n",
       "...                                                   ...           ...   \n",
       "100032                                   @heyfa64 Ù…Ù…Ù†ÙˆÙ†Ù…â¤  tq8v0qw6k277   \n",
       "100033                                @Milon10941303 â¤ï¸â¤ï¸  tupmhqbgvphc   \n",
       "100035                                      @kyloliqq ğŸ¤ğŸ¤ğŸ¤  u2wbxvbrvkcq   \n",
       "100039                                        @kiyowyd ğŸ¥°ğŸ¥µ  uesz20fesvvy   \n",
       "100040  @alperaltunoz @Turukku_ @FilipposBetsan1 @Akde...  sqyv84tsnn32   \n",
       "\n",
       "       Country         Region  \\\n",
       "2           mg     africa_sub   \n",
       "11          so   africa_north   \n",
       "17          tj   asia_central   \n",
       "18          dz   africa_north   \n",
       "21          bo  america_south   \n",
       "...        ...            ...   \n",
       "100032      ir   asia_central   \n",
       "100033      bd     asia_south   \n",
       "100035      hu    europe_east   \n",
       "100039      fi    europe_west   \n",
       "100040      gr    europe_west   \n",
       "\n",
       "                                              CleanedText Emojis  \\\n",
       "2         Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€      ğŸ’€ğŸ’€   \n",
       "11                         ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi  ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­   \n",
       "17                Hacer cosas de novios sin ser novios â™¥ï¸     â™¥ï¸   \n",
       "18       Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§ÙØ¶Ù„ ÙÙŠ Ø§Ø³ÙˆØ¡ Ø­...      ğŸ˜‚   \n",
       "21                             Lmaooo i felt that shit ğŸ¤£ğŸ’€     ğŸ¤£ğŸ’€   \n",
       "...                                                   ...    ...   \n",
       "100032                                            Ù…Ù…Ù†ÙˆÙ†Ù…â¤      â¤   \n",
       "100033                                               â¤ï¸â¤ï¸   â¤ï¸â¤ï¸   \n",
       "100035                                                ğŸ¤ğŸ¤ğŸ¤    ğŸ¤ğŸ¤ğŸ¤   \n",
       "100039                                                 ğŸ¥°ğŸ¥µ     ğŸ¥°ğŸ¥µ   \n",
       "100040               Stop watching Turkish soap opera.ğŸ¼ğŸ¦ƒ      ğŸ¼ğŸ¦ƒ   \n",
       "\n",
       "                                            SeparatedText EmojiSequences  \n",
       "2          Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€           [ğŸ’€ğŸ’€]  \n",
       "11                        ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­  ur ugly for that wallahi        [ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­]  \n",
       "17               Hacer cosas de novios sin ser novios â™¥ ï¸            [â™¥]  \n",
       "18      Ø§Ù†Ø§ Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ù„Ø§ ÙƒÙ† Ø£Ø±Ù‰ Ø£Ù† Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§ÙØ¶Ù„ ÙÙŠ Ø§Ø³ÙˆØ¡ Ø­Ø§...            [ğŸ˜‚]  \n",
       "21                             Lmaooo i felt that shit ğŸ¤£ğŸ’€           [ğŸ¤£ğŸ’€]  \n",
       "...                                                   ...            ...  \n",
       "100032                                           Ù…Ù…Ù†ÙˆÙ†Ù… â¤            [â¤]  \n",
       "100033                                            â¤ ï¸ â¤ ï¸         [â¤, â¤]  \n",
       "100035                                                ğŸ¤ğŸ¤ğŸ¤          [ğŸ¤ğŸ¤ğŸ¤]  \n",
       "100039                                                 ğŸ¥°ğŸ¥µ           [ğŸ¥°ğŸ¥µ]  \n",
       "100040               Stop watching Turkish soap opera. ğŸ¼ğŸ¦ƒ           [ğŸ¼ğŸ¦ƒ]  \n",
       "\n",
       "[20637 rows x 11 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to txt file (used for embeddings later)\n",
    "tweets_with_emojis.to_csv(\n",
    "    \"Data/2024-05-07-13-10/tweets_with_emojis.txt\",\n",
    "    columns=[\"SeparatedText\"],\n",
    "    header=False,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data to csv\n",
    "tweets_with_emojis.to_csv(\"Data/2024-05-07-13-10/tweets_with_emojis_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "data_dir = \"./Data/2024-05-07-13-10\"\n",
    "# Load data from CSV\n",
    "tweets_with_emojis = pd.read_csv(\n",
    "    f\"{data_dir}/tweets_with_emojis_dataframe.csv\", index_col=0\n",
    ")\n",
    "\n",
    "# Convert string representation of lists to actual lists\n",
    "tweets_with_emojis[\"EmojiSequences\"] = tweets_with_emojis[\"EmojiSequences\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 26M words\n",
      "Number of words:  796246\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  108542 lr:  0.000000 avg.loss:  1.053137 ETA:   0h 0m 0s 40.4% words/sec/thread:  112825 lr:  0.029813 avg.loss:  1.497900 ETA:   0h 1m41s\n",
      "Read 26M words\n",
      "Number of words:  796246\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  308815 lr:  0.000000 avg.loss:  1.347733 ETA:   0h 0m 0s  6.0% words/sec/thread:  302074 lr:  0.046991 avg.loss:  3.138034 ETA:   0h 0m59s1.347733 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "\n",
    "# Print test results\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "\n",
    "# Set data directory\n",
    "data_dir = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "data_file = os.path.join(data_dir, \"tweets_with_emojis.txt\")\n",
    "\n",
    "\n",
    "# SGNS model\n",
    "model = fasttext.train_unsupervised(\n",
    "    input=data_file,\n",
    "    model=\"skipgram\",\n",
    "    ws=4,\n",
    "    minCount=2,\n",
    "    minn=0,\n",
    "    maxn=0,\n",
    "    neg=5,\n",
    ")\n",
    "\n",
    "# Save bin format for SGNS\n",
    "model.save_model(os.path.join(data_dir, \"model_tweets_with_emojis_sgns.bin\"))\n",
    "\n",
    "\n",
    "# CBOW model\n",
    "model = fasttext.train_unsupervised(\n",
    "    input=data_file,\n",
    "    model=\"cbow\",\n",
    "    ws=4,\n",
    "    minCount=2,\n",
    "    minn=0,\n",
    "    maxn=0,\n",
    "    neg=5,\n",
    ")\n",
    "\n",
    "# Save bin format for CBOW\n",
    "model.save_model(os.path.join(data_dir, \"model_tweets_with_emojis_cbow.bin\"))\n",
    "\n",
    "\n",
    "# Convert SGNS from bin to vec format\n",
    "model = gensim.models.fasttext.load_facebook_vectors(\n",
    "    os.path.join(data_dir, \"model_tweets_with_emojis_sgns.bin\")\n",
    ")\n",
    "model.save_word2vec_format(os.path.join(data_dir, \"model_tweets_with_emojis_sgns.vec\"))\n",
    "\n",
    "\n",
    "# Convert CBOW from bin to vec format\n",
    "model = gensim.models.fasttext.load_facebook_vectors(\n",
    "    os.path.join(data_dir, \"model_tweets_with_emojis_cbow.bin\")\n",
    ")\n",
    "model.save_word2vec_format(os.path.join(data_dir, \"model_tweets_with_emojis_cbow.vec\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City Lang                 Time  \\\n",
      "2                tsiombe   fr  2021-12-06 15:50:41   \n",
      "11                 hobyo   en  2021-12-06 23:04:04   \n",
      "21        villa yapacani   en  2021-12-06 23:28:34   \n",
      "32          vangaindrano  und  2021-12-06 23:40:46   \n",
      "36         david-gorodok   en  2021-12-06 23:48:44   \n",
      "...                  ...  ...                  ...   \n",
      "12744226            soka   ja  2021-12-21 16:43:23   \n",
      "12744230        rumuruti  und  2021-12-21 16:43:24   \n",
      "12744299            soka   ja  2021-12-21 16:43:35   \n",
      "12744306  klosterneuburg   en  2021-12-21 16:43:36   \n",
      "12744323            soka   en  2021-12-21 16:43:38   \n",
      "\n",
      "                                                       Text       Geohash  \\\n",
      "2         Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€...  m5824frby6pt   \n",
      "11                  @M0zark0 ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi  t0fw2npnp7hn   \n",
      "21                  @andalechuey Lmaooo i felt that shit ğŸ¤£ğŸ’€  6sft52mvxsct   \n",
      "32                             ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ https://t.co/aUyQoey5Wc  m5cg5jm22ke2   \n",
      "36        Hello!  PaidTunes is a company that pays you w...  u972576t4r3b   \n",
      "...                                                     ...           ...   \n",
      "12744226                                 @yuyyumaki ãŠãŠãŠğŸ˜³ğŸ’—ğŸ’—ğŸ’—  xn77v2fkpxu8   \n",
      "12744230                      @ingasiania @DanAlumasa ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚  sb1cx2dc2q72   \n",
      "12744299                                  @reopakame é—‡ç³»ã™ãğŸ¤¤ğŸ’•  xn77v2fkpxu8   \n",
      "12744306  @cricketisle You could try it out! But for me ...  u2edg3xwjppu   \n",
      "12744323  See you in the Solice metaverse https://t.co/H...  xn77v2fkpxu8   \n",
      "\n",
      "         Country         Region  \\\n",
      "2             mg     africa_sub   \n",
      "11            so   africa_north   \n",
      "21            bo  america_south   \n",
      "32            mg     africa_sub   \n",
      "36            by    europe_east   \n",
      "...          ...            ...   \n",
      "12744226      jp      asia_east   \n",
      "12744230      ke     africa_sub   \n",
      "12744299      jp      asia_east   \n",
      "12744306      at    europe_west   \n",
      "12744323      jp      asia_east   \n",
      "\n",
      "                                                CleanedText Emojis  \\\n",
      "2           Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€      ğŸ’€ğŸ’€   \n",
      "11                           ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ ur ugly for that wallahi  ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­   \n",
      "21                               Lmaooo i felt that shit ğŸ¤£ğŸ’€     ğŸ¤£ğŸ’€   \n",
      "32                                                    ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­    ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­   \n",
      "36        Hello!  PaidTunes is a company that pays you w...     ğŸ’°ğŸ’°   \n",
      "...                                                     ...    ...   \n",
      "12744226                                            ãŠãŠãŠğŸ˜³ğŸ’—ğŸ’—ğŸ’—   ğŸ˜³ğŸ’—ğŸ’—ğŸ’—   \n",
      "12744230                                              ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚  ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚   \n",
      "12744299                                             é—‡ç³»ã™ãğŸ¤¤ğŸ’•     ğŸ¤¤ğŸ’•   \n",
      "12744306   You could try it out! But for me itâ€˜s also th...     ğŸ˜ŠğŸ˜‰   \n",
      "12744323  See you in the Solice metaverse  #soliceio #so...     ğŸ‘ğŸ‘   \n",
      "\n",
      "                                              SeparatedText EmojiSequences  \n",
      "2            Eh le poto il est Ã  lâ€™aise avec sa question ğŸ’€ğŸ’€           [ğŸ’€ğŸ’€]  \n",
      "11                          ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­  ur ugly for that wallahi        [ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­]  \n",
      "21                               Lmaooo i felt that shit ğŸ¤£ğŸ’€           [ğŸ¤£ğŸ’€]  \n",
      "32                                                     ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­         [ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­]  \n",
      "36        Hello!  PaidTunes is a company that pays you w...           [ğŸ’°ğŸ’°]  \n",
      "...                                                     ...            ...  \n",
      "12744226                                           ãŠãŠãŠ ğŸ˜³ğŸ’—ğŸ’—ğŸ’—         [ğŸ˜³ğŸ’—ğŸ’—ğŸ’—]  \n",
      "12744230                                              ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚        [ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚]  \n",
      "12744299                                            é—‡ç³»ã™ã ğŸ¤¤ğŸ’•           [ğŸ¤¤ğŸ’•]  \n",
      "12744306  You could try it out! But for me itâ€˜s also tha...           [ğŸ˜ŠğŸ˜‰]  \n",
      "12744323  See you in the Solice metaverse  #soliceio #so...           [ğŸ‘ğŸ‘]  \n",
      "\n",
      "[1107101 rows x 11 columns]\n",
      "FastTextKeyedVectors<vector_size=100, 796246 keys>\n",
      "[[ 0.41380402 -0.11446256 -0.7173146  ...  0.53497213  0.5812335\n",
      "   0.32401615]\n",
      " [ 0.45882672  0.37555623 -0.5633557  ...  0.10746752  0.18964337\n",
      "   0.48710862]\n",
      " [ 0.2746834   0.14251009 -0.21636261 ...  0.27197945  0.24982107\n",
      "  -0.19502364]\n",
      " ...\n",
      " [-0.12353732  0.12085461  0.13683707 ... -0.20110886  0.042679\n",
      "  -0.34974697]\n",
      " [ 0.28422418 -0.10544135 -0.40650862 ...  0.23608352  0.17467871\n",
      "  -0.5783039 ]\n",
      " [-0.34844565 -0.10982028  0.04284889 ...  0.6962104   0.78057766\n",
      "  -0.62641037]]\n",
      "       EmojiSequences Topic\n",
      "922344            ğŸ¤©ğŸ¤©ğŸ¤©     0\n",
      "216442             ğŸ˜ğŸ¥º     0\n",
      "707227             ğŸ˜ğŸ”¥     0\n",
      "332597             ğŸ˜ğŸ”¥     0\n",
      "22277             ğŸ‘ŒğŸ‘ŒğŸ‘Œ     0\n",
      "...               ...   ...\n",
      "828018       ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£     9\n",
      "958233         ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£     9\n",
      "339586          ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£     9\n",
      "827995          ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£     9\n",
      "387824        ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£     9\n",
      "\n",
      "[990194 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import emoji\n",
    "\n",
    "figure(figsize=(12, 12), dpi=150)\n",
    "\n",
    "# Set file names and location\n",
    "root = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "sg_file = os.path.join(root, \"model_tweets_with_emojis_sgns.bin\")\n",
    "cbow_file = os.path.join(root, \"model_tweets_with_emojis_cbow.bin\")\n",
    "\n",
    "# Load vocab features (tweets )\n",
    "vocab_df = tweets_with_emojis[\n",
    "    tweets_with_emojis[\"EmojiSequences\"].apply(\n",
    "        lambda sequences: all(emoji.emoji_count(seq) > 1 for seq in sequences)\n",
    "    )\n",
    "]\n",
    "print(vocab_df)\n",
    "\n",
    "# Load embedding (change file to change type)\n",
    "model = gensim.models.fasttext.load_facebook_vectors(sg_file)\n",
    "model_type = \"cbow\"\n",
    "print(model)\n",
    "\n",
    "# Get embeddings for the vocab in the file\n",
    "embeddings = []\n",
    "emoji_sequences_with_embeddings = []\n",
    "for emoji_sequences_list in vocab_df.loc[:, \"EmojiSequences\"].values:\n",
    "    for emoji_sequence in emoji_sequences_list:\n",
    "        try:\n",
    "            embeddings.append(model[emoji_sequence])\n",
    "            emoji_sequences_with_embeddings.append(emoji_sequence)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "emoji_seq_with_embeddings_df = pd.DataFrame(\n",
    "    {\"EmojiSequences\": emoji_sequences_with_embeddings}\n",
    ")\n",
    "vocab_df = emoji_seq_with_embeddings_df\n",
    "\n",
    "\n",
    "# Now individual arrays into single matrix (note shifting types!)\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(embeddings)\n",
    "\n",
    "# Cluster words\n",
    "cluster = KMeans(n_clusters=20, init=\"k-means++\", max_iter=300)\n",
    "\n",
    "# Fit cluster (i.e., run it)\n",
    "cluster.fit(embeddings)\n",
    "\n",
    "# Now get cluster labels\n",
    "vocab_df.loc[:, \"Topic\"] = [str(x) for x in cluster.labels_]\n",
    "vocab_df.sort_values(\"Topic\", inplace=True)\n",
    "print(vocab_df)\n",
    "\n",
    "# Now save and visualize clusters\n",
    "root = os.path.join(\".\", \"Clusters/2024-05-08-13-20\")\n",
    "for cluster, cluster_df in vocab_df.groupby(\"Topic\"):\n",
    "    # Get vocab items in the model\n",
    "    vocab = set(\n",
    "        [\n",
    "            seq\n",
    "            for seq in cluster_df.loc[:, \"EmojiSequences\"].values\n",
    "            if not np.all(model[seq] == 0)  # Make sure embedding is not 0\n",
    "        ]\n",
    "    )\n",
    "    save = True\n",
    "\n",
    "    if vocab == []:\n",
    "        continue\n",
    "\n",
    "    # Now find the distance of each word from the centroid\n",
    "    cluster_shape = model.rank_by_centrality(vocab, use_norm=True)\n",
    "\n",
    "    cluster_shape = pd.DataFrame(cluster_shape, columns=[\"Similarity\", \"EmojiSequence\"])\n",
    "\n",
    "    # print(cluster_shape)\n",
    "    cluster_shape.to_csv(\n",
    "        os.path.join(root, \"Clusters.\" + model_type + \".\" + str(cluster) + \".csv\")\n",
    "    )\n",
    "\n",
    "    # Get distance from the center word\n",
    "    try:\n",
    "        distances_center = model.distances(\n",
    "            cluster_shape.iloc[0, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "    try:\n",
    "        distances_edge = model.distances(\n",
    "            cluster_shape.iloc[-1, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "\n",
    "    # Just to catch any key errors for missing words\n",
    "    if save == True:\n",
    "        # Make dataframe\n",
    "        cluster_graph = pd.DataFrame([vocab, distances_center, distances_edge]).T\n",
    "        cluster_graph.columns = [\"Word\", \"Distance from Center\", \"Distance from Edge\"]\n",
    "\n",
    "        # Make graph of the cluster\n",
    "        ax = sns.scatterplot(\n",
    "            data=cluster_graph, x=\"Distance from Center\", y=\"Distance from Edge\"\n",
    "        )\n",
    "\n",
    "        # Add words to label points\n",
    "        for row in cluster_graph.itertuples():\n",
    "            emoji_sequence = row[1]\n",
    "            x = row[2]\n",
    "            y = row[3]\n",
    "\n",
    "            # plt.annotate(\n",
    "            #     emoji.demojize(emoji_sequence).strip(\":\"),\n",
    "            #     (x, y),\n",
    "            #     textcoords=\"offset points\",\n",
    "            #     xytext=(0, 10),\n",
    "            #     ha=\"center\",\n",
    "            # )\n",
    "\n",
    "        # Save\n",
    "        plt.savefig(\n",
    "            os.path.join(root, \"Clusters.\" + model_type + \".\" + str(cluster) + \".png\"),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=150,\n",
    "        )\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with both individual emojis and sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextKeyedVectors<vector_size=100, 796246 keys>\n",
      "[[ 0.41380402 -0.11446256 -0.7173146  ...  0.53497213  0.5812335\n",
      "   0.32401615]\n",
      " [ 0.45882672  0.37555623 -0.5633557  ...  0.10746752  0.18964337\n",
      "   0.48710862]\n",
      " [ 0.23250706  0.81500727  1.166513   ...  0.3877129  -0.26803163\n",
      "  -1.1895267 ]\n",
      " ...\n",
      " [ 0.27002043 -1.1184332   0.01037823 ... -0.4062101  -0.22688912\n",
      "   0.53178984]\n",
      " [-1.138427   -0.09576458  0.51226735 ... -0.24816284  1.3126827\n",
      "  -1.0916843 ]\n",
      " [ 0.27747092  0.02784789  0.34089074 ... -0.24584675  0.31233457\n",
      "  -0.3711467 ]]\n",
      "        EmojiSequences Topic\n",
      "2074828              ğŸ‘Œ     0\n",
      "1498618              ğŸ‘Œ     0\n",
      "3397247              ğŸ‘Œ     0\n",
      "2909770              ğŸ‘Œ     0\n",
      "878794               ğŸ‘Œ     0\n",
      "...                ...   ...\n",
      "3462411              ğŸ’›     9\n",
      "2093404              ğŸ§¡     9\n",
      "2902256              ğŸ’œ     9\n",
      "3347091              ğŸ–¤     9\n",
      "1583919              ğŸ’œ     9\n",
      "\n",
      "[3528540 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import emoji\n",
    "\n",
    "figure(figsize=(12, 12), dpi=150)\n",
    "\n",
    "# Set file names and location\n",
    "root = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "sg_file = os.path.join(root, \"model_tweets_with_emojis_sgns.bin\")\n",
    "cbow_file = os.path.join(root, \"model_tweets_with_emojis_cbow.bin\")\n",
    "\n",
    "# Load vocab features\n",
    "vocab_df = tweets_with_emojis\n",
    "\n",
    "# Load embedding (change file to change type)\n",
    "model = gensim.models.fasttext.load_facebook_vectors(sg_file)\n",
    "model_type = \"cbow\"\n",
    "print(model)\n",
    "\n",
    "# Get embeddings for the vocab in the file\n",
    "embeddings = []\n",
    "emoji_sequences_with_embeddings = []\n",
    "for emoji_sequences_list in vocab_df.loc[:, \"EmojiSequences\"].values:\n",
    "    for emoji_sequence in emoji_sequences_list:\n",
    "        try:\n",
    "            embeddings.append(model[emoji_sequence])\n",
    "            emoji_sequences_with_embeddings.append(emoji_sequence)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "emoji_seq_with_embeddings_df = pd.DataFrame(\n",
    "    {\"EmojiSequences\": emoji_sequences_with_embeddings}\n",
    ")\n",
    "vocab_df = emoji_seq_with_embeddings_df\n",
    "\n",
    "\n",
    "# Now individual arrays into single matrix (note shifting types!)\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(embeddings)\n",
    "\n",
    "# Cluster words\n",
    "cluster = KMeans(n_clusters=20, init=\"k-means++\", max_iter=300)\n",
    "\n",
    "# Fit cluster (i.e., run it)\n",
    "cluster.fit(embeddings)\n",
    "\n",
    "# Now get cluster labels\n",
    "vocab_df.loc[:, \"Topic\"] = [str(x) for x in cluster.labels_]\n",
    "vocab_df.sort_values(\"Topic\", inplace=True)\n",
    "print(vocab_df)\n",
    "\n",
    "# Now save and visualize clusters\n",
    "root = os.path.join(\".\", \"Clusters/2024-05-09-12-05\")\n",
    "for cluster, cluster_df in vocab_df.groupby(\"Topic\"):\n",
    "    # Get the vocab items that are in model\n",
    "    vocab = set(\n",
    "        [\n",
    "            seq\n",
    "            for seq in cluster_df.loc[:, \"EmojiSequences\"].values\n",
    "            if not np.all(model[seq] == 0)  # Make sure embedding is not 0\n",
    "        ]\n",
    "    )\n",
    "    save = True\n",
    "\n",
    "    if vocab == []:\n",
    "        continue\n",
    "\n",
    "    # Now find the distance of each word from the centroid\n",
    "    cluster_shape = model.rank_by_centrality(vocab, use_norm=True)\n",
    "\n",
    "    cluster_shape = pd.DataFrame(cluster_shape, columns=[\"Similarity\", \"EmojiSequence\"])\n",
    "\n",
    "    # print(cluster_shape)\n",
    "    cluster_shape.to_csv(\n",
    "        os.path.join(\n",
    "            root, \"Clusters.IndivAndSeq.\" + model_type + \".\" + str(cluster) + \".csv\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get distance from the center word\n",
    "    try:\n",
    "        distances_center = model.distances(\n",
    "            cluster_shape.iloc[0, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "    try:\n",
    "        distances_edge = model.distances(\n",
    "            cluster_shape.iloc[-1, 1], other_words=tuple(vocab)\n",
    "        )\n",
    "    except:\n",
    "        save = False\n",
    "\n",
    "    # Just to catch any key errors for missing words\n",
    "    if save == True:\n",
    "        # Make dataframe\n",
    "        cluster_graph = pd.DataFrame([vocab, distances_center, distances_edge]).T\n",
    "        cluster_graph.columns = [\"Word\", \"Distance from Center\", \"Distance from Edge\"]\n",
    "\n",
    "        # Make graph of the cluster\n",
    "        ax = sns.scatterplot(\n",
    "            data=cluster_graph, x=\"Distance from Center\", y=\"Distance from Edge\"\n",
    "        )\n",
    "\n",
    "        # Add words to label points\n",
    "        for row in cluster_graph.itertuples():\n",
    "            emoji_sequence = row[1]\n",
    "            x = row[2]\n",
    "            y = row[3]\n",
    "\n",
    "        # Save\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                root, \"Clusters.IndivAndSeq.\" + model_type + \".\" + str(cluster) + \".png\"\n",
    "            ),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=150,\n",
    "        )\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 1.905306602743449\n"
     ]
    }
   ],
   "source": [
    "# Flatten list of sequences\n",
    "all_sequences = [sequence for sequences in tweets_with_emojis['EmojiSequences'] for sequence in sequences]\n",
    "all_sequences_at_least_2 = [sequence for sequence in all_sequences if emoji.emoji_count(sequence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.339016947504073"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([emoji.emoji_count(sequence) for sequence in all_sequences_at_least_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([emoji.emoji_count(sequence) for sequence in all_sequences_at_least_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2992375"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tweets used?\n",
    "len(tweets_with_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244654"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique sequences of at least 2?\n",
    "len(set(all_sequences_at_least_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique individual emojis?\n",
    "all_emojis_from_data = sorted([''.join(get_emojis(x)) for x in list(set(''.join(tweets_with_emojis.loc[:,\"Emoji\"].values)))])\n",
    "all_unique_emojis_from_data = set(all_emojis_from_data)\n",
    "len(all_unique_emojis_from_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common sequences:\n",
      "ğŸ˜‚ğŸ˜‚ğŸ˜‚ : 57871\n",
      "ğŸ˜‚ğŸ˜‚ : 56719\n",
      "ğŸ¤£ğŸ¤£ğŸ¤£ : 33592\n",
      "ğŸ¤£ğŸ¤£ : 24275\n",
      "ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ : 24073\n",
      "ğŸ˜­ğŸ˜­ğŸ˜­ : 20088\n",
      "ğŸ˜­ğŸ˜­ : 19998\n",
      "ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ : 15248\n",
      "ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ : 11023\n",
      "ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ : 8581\n",
      "10 most common emojis:\n",
      "â¤ : 183941\n",
      "ğŸ˜‚ : 153454\n",
      "ğŸ˜­ : 68989\n",
      "ğŸ¤£ : 68818\n",
      "ğŸ¥º : 41917\n",
      "ğŸ˜ : 40050\n",
      "ğŸ˜… : 37134\n",
      "â™¥ : 36839\n",
      "ğŸ˜Š : 33679\n",
      "ğŸ¥° : 31201\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each sequence\n",
    "sequence_counter = Counter(all_sequences_at_least_2)\n",
    "most_common_sequences = sequence_counter.most_common(10)\n",
    "\n",
    "print(\"10 most common sequences:\")\n",
    "for sequence, count in most_common_sequences:\n",
    "    if emoji.emoji_count(sequence) >= 2:\n",
    "        print(sequence, \":\", count)\n",
    "\n",
    "\n",
    "# Count occurrences of each emoji\n",
    "sequence_counter = Counter([seq for seq in all_sequences if len(get_emojis(seq)) == 1])\n",
    "most_common_emojis = sequence_counter.most_common(10)\n",
    "\n",
    "print(\"10 most common emojis:\")\n",
    "for common_emoji, count in most_common_emojis:\n",
    "    print(common_emoji, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fasttext.FastText._FastText object at 0x279b882c0>\n",
      "getting distances for ğŸ™ŒğŸ™ƒ\n",
      "getting distances for ğŸ‘âš½\n",
      "getting distances for ğŸ‡µğŸ‡¹ğŸ‡µğŸ‡¹ğŸ‡µğŸ‡¹\n",
      "getting distances for ğŸ‚ğŸ¾ğŸŒ²\n",
      "getting distances for ğŸ’ªğŸ¼ğŸ¤œğŸ¼ğŸ¤›ğŸ¼\n",
      "getting distances for ğŸ˜ğŸ‘\n",
      "getting distances for ğŸ˜­ğŸ˜­ğŸ˜¢ğŸ˜¢\n",
      "getting distances for ğŸ¤ªğŸ‘Œ\n",
      "getting distances for ğŸ‘©ğŸ¾â€â¤\n",
      "getting distances for ğŸ˜·ğŸ¦ ğŸ’‰\n",
      "getting distances for ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ’”ğŸ’”ğŸ’”\n",
      "getting distances for ğŸ‘ğŸ¥¶\n",
      "getting distances for ğŸ˜‚ğŸ˜‚ğŸ˜­ğŸ˜­\"\n",
      "getting distances for ğŸ˜¬ğŸ¤”\"\n",
      "getting distances for ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿\n",
      "getting distances for ğŸ¥²ğŸ’•ğŸ’•\n",
      "getting distances for ğŸŒ³ğŸŒ·\n",
      "getting distances for ğŸ¶ğŸ’©\n",
      "getting distances for ğŸ¤©ğŸ‘Œ\"\n",
      "getting distances for ğŸ¤£ğŸ¤£ğŸ™\n",
      "       Similarity Emoji Sequence Target Emoji Sequence\n",
      "28784    1.000000             ğŸ™ŒğŸ™ƒ                    ğŸ™ŒğŸ™ƒ\n",
      "29581    0.934571            âœ¨ğŸ™Œ\"                    ğŸ™ŒğŸ™ƒ\n",
      "54220    0.925216             ğŸ˜³ğŸ›                    ğŸ™ŒğŸ™ƒ\n",
      "38791    0.919492            ğŸ¤—ğŸ‘ğŸ¼                    ğŸ™ŒğŸ™ƒ\n",
      "25509    0.918676            ğŸ˜ğŸ™Œ\"                    ğŸ™ŒğŸ™ƒ\n",
      "...           ...            ...                   ...\n",
      "44593    0.879122           ğŸ¤©ğŸ‰ğŸğŸ’µ                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "24998    0.877754             ğŸ˜©ğŸš®                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "39954    0.877603             ğŸ˜‚ğŸ                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "34584    0.877298           ğŸ“ ğŸ“ ğŸ“ ğŸ“                    ğŸ¤£ğŸ¤£ğŸ™\n",
      "41165    0.877155            ğŸ¥ºğŸ™„ğŸ˜‚                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fasttext.FastText._FastText object at 0x277abf3e0>\n",
      "getting distances for ğŸ™ŒğŸ™ƒ\n",
      "getting distances for ğŸ‘âš½\n",
      "getting distances for ğŸ‡µğŸ‡¹ğŸ‡µğŸ‡¹ğŸ‡µğŸ‡¹\n",
      "getting distances for ğŸ‚ğŸ¾ğŸŒ²\n",
      "getting distances for ğŸ’ªğŸ¼ğŸ¤œğŸ¼ğŸ¤›ğŸ¼\n",
      "getting distances for ğŸ˜ğŸ‘\n",
      "getting distances for ğŸ˜­ğŸ˜­ğŸ˜¢ğŸ˜¢\n",
      "getting distances for ğŸ¤ªğŸ‘Œ\n",
      "getting distances for ğŸ‘©ğŸ¾â€â¤\n",
      "getting distances for ğŸ˜·ğŸ¦ ğŸ’‰\n",
      "getting distances for ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ’”ğŸ’”ğŸ’”\n",
      "getting distances for ğŸ‘ğŸ¥¶\n",
      "getting distances for ğŸ˜‚ğŸ˜‚ğŸ˜­ğŸ˜­\"\n",
      "getting distances for ğŸ˜¬ğŸ¤”\"\n",
      "getting distances for ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿ğŸ¿\n",
      "getting distances for ğŸ¥²ğŸ’•ğŸ’•\n",
      "getting distances for ğŸŒ³ğŸŒ·\n",
      "getting distances for ğŸ¶ğŸ’©\n",
      "getting distances for ğŸ¤©ğŸ‘Œ\"\n",
      "getting distances for ğŸ¤£ğŸ¤£ğŸ™\n",
      "       Similarity Emoji Sequence Target Emoji Sequence\n",
      "28784    1.000000             ğŸ™ŒğŸ™ƒ                    ğŸ™ŒğŸ™ƒ\n",
      "6854     0.863498             â¤ğŸ¶                    ğŸ™ŒğŸ™ƒ\n",
      "6872     0.859594            ğŸ¥šğŸ¥šğŸ¥š                    ğŸ™ŒğŸ™ƒ\n",
      "13799    0.859109            ğŸ¤—ğŸ˜\"                    ğŸ™ŒğŸ™ƒ\n",
      "10228    0.850270             ğŸ˜ˆğŸ¤¤                    ğŸ™ŒğŸ™ƒ\n",
      "...           ...            ...                   ...\n",
      "19272    0.856314            ğŸ™ğŸ¼ğŸ€                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "10822    0.854694            ğŸ˜­ğŸ˜”\"                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "11982    0.854626            ğŸ¤—ğŸ¤—ğŸ¥°                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "6178     0.853978             ğŸ’¯ğŸ¤                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "4159     0.853913             ğŸ¥°ğŸ˜­                   ğŸ¤£ğŸ¤£ğŸ™\n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(12, 6), dpi=150)\n",
    "\n",
    "# Get nearest neighbors for target word (i.e., target emoji)\n",
    "def get_distances(model, target, vocab):\n",
    "    # Get embedding for target emoji\n",
    "    target_embedding = model[target].reshape(1, -1)\n",
    "\n",
    "    # Get cosine similarity for each word in vocab\n",
    "    distances = []\n",
    "    for word in vocab:\n",
    "        similarity = cosine_similarity(target_embedding, model[word].reshape(1, -1))[0][0]\n",
    "        distances.append([similarity, word])\n",
    "\n",
    "    # Make dataframe\n",
    "    results_df = pd.DataFrame(distances, columns=[\"Similarity\", \"Emoji Sequence\"])\n",
    "    results_df.sort_values(\"Similarity\", ascending=False, inplace=True)\n",
    "    results_df = results_df.head(20)\n",
    "    results_df.loc[:,\"Target Emoji Sequence\"] = target\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Set file names and location\n",
    "data_dir = os.path.join(\".\", \"Data/2024-05-07-13-10\")\n",
    "sg_file = os.path.join(data_dir, \"model_tweets_with_emojis_sgns.bin\")\n",
    "cbow_file = os.path.join(data_dir, \"model_tweets_with_emojis_cbow.bin\")\n",
    "\n",
    "\n",
    "# Get 20 random emoji sequences\n",
    "# words = random.sample(tweets_with_emojis.loc[:,\"Emojis\"], 20)\n",
    "# words = get_emoji_sample_with_categories(n=20) \n",
    "seq_in_model = [w for w in model.get_words() if emoji.emoji_count(w) > 1]\n",
    "words = random.sample(seq_in_model, 20)\n",
    "\n",
    "# Generate id for file\n",
    "tag = str(random.randint(1,1000))\n",
    "\n",
    "# For each type of embedding\n",
    "for embedding_file in [sg_file, cbow_file]:\n",
    "    # Get model_type\n",
    "    if \"sgns\" in embedding_file:\n",
    "        model_type = \"sg\"\n",
    "    elif \"cbow\" in embedding_file:\n",
    "        model_type = \"cbow\"\n",
    "\n",
    "    # Load embedding \n",
    "    model = fasttext.load_model(embedding_file)\n",
    "    print(model)\n",
    "\n",
    "    # Iterate over words (emojis from sample)\n",
    "    stack = []\n",
    "    for emoji_sequence in words:\n",
    "        print(f\"getting distances for {emoji_sequence}\")\n",
    "        # emojis_in_same_category = all_emojis_by_category[all_emojis_by_category['Category'] == the_emoji_category]\n",
    "        nearest = get_distances(model, emoji_sequence, seq_in_model)\n",
    "        stack.append(nearest)\n",
    "\n",
    "    # Concat\n",
    "    nearest_df = pd.concat(stack)\n",
    "    print(nearest_df)\n",
    "    \n",
    "    # Save\n",
    "    file = f\"Neighbors/2024-05-08-07-10/TweetsWithEmojis.Neighbors.{tag}.{model_type}\"\n",
    "    nearest_df.to_csv(file+\".csv\")\n",
    "    \n",
    "    # Graph\n",
    "    ax = sns.stripplot(data=nearest_df, x=\"Target Emoji Sequence\", hue=\"Target Emoji Sequence\", y=\"Similarity\", jitter=True, size=2)\n",
    "\n",
    "    # Hide word labels\n",
    "    frame = plt.gca()\n",
    "    # frame.axes.xaxis.set_ticklabels([f\"{emoji.demojize(w).strip(\":\")} {w}\" for w in words['Emoji']])\n",
    "    frame.axes.xaxis.set_ticklabels([])\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "    plt.savefig(file+\".neighborhoods.png\", dpi = 150, bbox_inches = \"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247621"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.load_model(cbow_file)\n",
    "# get_distances(model, ğŸ¥±ğŸ¤¢,ğŸ¶ğŸ¶ğŸ˜Š)\n",
    "word = \"ğŸ¥±ğŸ¤¢\"\n",
    "target = \"ğŸ¶ğŸ¶ğŸ˜Š\"\n",
    "target_embedding = model[target].reshape(1, -1)\n",
    "cosine_similarity(target_embedding, model[word].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60231"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([w for w in model.get_words() if emoji.emoji_count(w) > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ling413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
